\setcounter{page}{1}
\pagenumbering{arabic}
\setlength{\parindent}{0.0in}

# Introduction
<!--- What is machine learning? How does it impact the world? -->
<!-- An increasing number of new and exciting machine learning (ML) applications is taking the modern world by storm. Machine learning is used to predict traffic in Google Maps, recommend Movies on Netflix, assess the situation around self-driving cars, detect spam in E-Mails, etc. The machine learning toolbox is a collection of methods where models are not explicitly programmed but learn from data instead. There is no evidence that this trend will stop any time soon. Every day we find exciting and complex applications that require advanced ML models, but with great power comes great responsibility. -->

<!--- What is the problem? -->
<!-- Robustness: https://vectorinstitute.ai/2022/03/29/machine-learning-robustness-new-challenges-and-approaches/ -->
<!-- "Robustness" refers to a model's ability to resist being fooled. -->
With the ever-increasing capabilities and importance of machine learning (ML) models at the core of many applications, there is a need to prove their robustness. Robustness in artificial intelligence (AI) denotes one of the most crucial research areas in machine learning. A model is considered to be robust if the model's performance in real-world situations deviates only marginally from the test performance, even if one or more of the features changes drastically due to unforeseen circumstances. Put differently: Robustness refers to a model's ability to resist being fooled. This definition is why every machine learning development process involves dividing the dataset into training, validation and test splits during the development phase to minimize the bias and variance of the model. The training split selects a suitable machine learning model with initial hyperparameters. The validation split tunes the hyperparameters to reach a performance optima. The performance on the test split is the final result to be published. Many machines learning beginner problems presented in university or one of the gazillions of excellent online courses offer one huge benefit. Usually, the data for the classic training, validation and test splits are sampled from the same data distribution. This property of the sampling process offers a massive advantage in obtaining satisfying results, which is suitable for a gentle introduction to the fascinating world of machine learning. Nevertheless, in the real-world scenario, there is always a risk involved that the data used for the training, validation and test splits does not accurately reflect the data available by the deployed model. This distribution shift between the data used during the development of the model and the deployed model is designated as a covariate shift. A covariate shift hides the dangerous problem that a model seems to work in the lab for its intended task. It achieves high performance in the training, validation and test splits but fails in the real-world.

<!--- What is our solution approach? -->
<!--- Describe the idea -->
One obvious solution to this problem is to ensure that the data for the development of the model reflects real-world data perfectly. Since this is by no means a trivial task, this work presents a novel solution that takes a different approach: The caption-based explainable artificial intelligence (XAI) method. In an idealized world, the model communicates which concept it focuses on in written text. The explainable artificial intelligence method presented in this work attempts to obtain a caption-based explanation for a standalone model to be explained. A biased dataset is introduced to create a biased standalone model. This biased dataset contains a covariate shift between the train/validation/test datasets (Representing the available data during the model development) and the real-world dataset (Representing real-world data after deployment). The objective of the XAI method is to identify that the standalone model to be explained focuses on the bias instead of learning the actual task. This finding enables the developer to improve the model and increase its robustness accordingly before deploying it into the real-world.

<!--- Overview chapters -->
The development of the caption-based explainable AI method requires many different components. The following chapters include a detailed description of all involved components:

- \*@sec:problem-description defines current challenges and opportunities in the world of robust machine learning. The original project description and its milestones are also included.
- \*@sec:explainable-ai is focused on existing explainable AI methods and the discussion around their advantages and disadvantages.
- \*@sec:contrastive-language-image-pre-training explains the contrastive language-image pre-training (CLIP) model, a core component of the caption-based explainable AI method.
- \*@sec:dataset introduces the dataset to train, validate and test the standalone model. Furthermore, the purposely introduced bias in the dataset is explained in detail.
- \*@sec:standalone-model presents the selection process for a suitable biased machine learning model. The training, validation and test split performance demonstrate a biased standalone model, which is a perfect candidate to demonstrate the caption-based explainable AI method.
- \*@sec:caption-based-explainable-ai provides an overview of how the caption-based explainable AI method works. All involved components are explained in detail.
- \*@sec:results evaluates the performance, which allows for a discussion on the suitability of the caption-based explainable AI method for a given situation.
- \*@sec:conclusion consolidates all ideas from the previous chapters, summarizes the gained knowledge from this project, discusses open questions and shares some advice on future approaches on this topic.
- \*@sec:closing-words contains a personal reflection of this work.
