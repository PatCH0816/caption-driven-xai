# Results
The most important question of the presented work is: "Is it reasonable to use CLIP to explain a ResNet-50 neural network?". This work presents a caption-based explainable AI model introduced in \*@sec:caption-based-explainable-ai to answer this question. The model to be explained possesses a ResNet-50 architecture as specified in \*@sec:standalone-model and is trained on the dataset introduced in \*@sec:dataset. The following sections demonstrate the results and dive deep into the intermediate steps of the caption-based explainable AI model. If nothing else is mentioned, all figures in this chapter are created using the parameters in \*@tbl:results_figure_parameters.

|Parameters             | Value
|-                      | -          
|XAI method             | Caption-based explainable AI model
|Model to be explained  | Standalone model
|Dataset                | Test dataset
|Color digits           | Yes
|Batch size             | 25
Table: The parameters in use to obtain the figures in this chapter. {#tbl:results_figure_parameters}

## Initial situation
The ResNet-50 standalone model introduced in \*@sec:standalone-model is trained on the dataset introduced in \*@sec:dataset. The training, validation and test learning curves indicate a low bias, low variance model as shown in \*@fig:is_performance_biased_without_test_fool. A machine learning developer could be tempted to deploy this model into the real-world environment.

![This figure illustrates the low bias, low variance learning progress of the finetuned ResNet-50 model on the biased color MNIST training, validation and test datasets during the model development process.](source/figures/performance_biased_without_test_fool.png "Training, validation and test learning curves from standalone ResNet-50 on custom MNIST dataset for binary classification."){#fig:is_performance_biased_without_test_fool width=50%}

If the ResNet-50 standalone model introduced in \*@sec:standalone-model shown would be deployed into the real-world environment, its accuracy would drop significantly and demonstrate random behavior as shown in \*@fig:is_performance_biased_with_test_fool as explained in detail in \*@sec:standalone-model. The caption-based explainable AI model supports machine learning developers to detect such a problem before deployment. The findings from the caption-based explainable AI model can then be used to remove the bias and train a more robust unbiased standalone model.

![This figure illustrates the low bias, low variance learning progress of the finetuned ResNet-50 model on the biased color MNIST training, validation and test datasets during the model development process. Additionally, the real-world curve demonstrates the poor performance simulated in the real-world environment.](source/figures/performance_biased_with_test_fool.png "Training, validation, test and real-world learning curves from standalone ResNet-50 on custom MNIST dataset for binary classification."){#fig:is_performance_biased_with_test_fool width=50%}

## Standalone model statistics
One crucial assumption for the caption-based explainable AI model is that the activation maps statistics of the standalone model are gaussians, as mentioned in \*@sec:compute-statistics. \*@fig:activations_gaussian_mean_std_standalone demonstrates that the distribution of means and standard deviations over all activation maps in a non-cherry-picked layer 39 are gaussians. This observation is accurate for any layer. Therefore, the distributions can be described with the mean and the standard deviation.

![Gaussian activations statistics from random layer 39 of the standalone model. Activations from other layers are also gaussians.](source/figures/activations_gaussian_mean_std_standalone.png "Gaussian activations statistics from random layer 39 of the standalone model."){#fig:activations_gaussian_mean_std_standalone width=75%}

The same assumption about the gaussian distribution of means and standard deviations needs to be fullfilled by the CLIP image encoder. \*@fig:activations_gaussian_mean_std_clip demonstrates that the distribution of means and standard deviations over all activation maps in a non-cherry picked layer 39 are gaussians. This observation is accurate for any layer. Therefore, the distributions can be described with the mean and the standard deviation.

![Gaussian activations statistics from random layer 39 of the CLIP model. Activations from other layers are also gaussians.](source/figures/activations_gaussian_mean_std_clip.png "Gaussian activations statistics from random layer 39 of the clip model."){#fig:activations_gaussian_mean_std_clip width=75%}

## Standalone model activation matching
As introduced in the \*@sec:compute-statistics, there are 49 convolutional layers in the standalone model, which result in a total of 22720 activation maps. On the other hand, 4 out of the 51 convolutional layers in the CLIP image encoder are available for the activation matching process. These four CLIP layers result in a total of 3840 activation maps. According to the activation matching process as explained in \*@sec:activation-matching, the most significant 3840 scores between the CLIP image encoder and the standalone model activation maps are selected to be swapped as shown in \*@fig:activation_matching_stages. This results in swapping around $\frac{3840}{22720} = 16.9\%$ of all available activation maps from the standalone model to the CLIP image encoder. Applying a moving average with an empirical window size of 42 reveals a structure that looks like four stages. Recall that the four layers swapped in the CLIP image encoder are located in the last four out of five stages of the ResNet architecture, as shown in \*@fig:activation_matching. This pattern of the moving average is expected since it confirms that the activation matching process tends to swap layers between similar stages of the standalone model and the CLIP image encoder on average.

![Activation matching between the CLIP image encoder and the standalone model activation maps. This matching process selects 3840 out of 22720 activation maps from the standalone model. The moving average indicates a tendency to choose activation maps from the same ResNet stage.](source/figures/activation_matching_stages.png "Activation matching between the CLIP image encoder and the standalone model activation maps."){#fig:activation_matching_stages width=75%}

\*@fig:activation_matching_scores shows the matching scores between the 3840 selected activation maps to be swapped from the standalone model to the CLIP image encoder. The scores of the activation maps are rather large near the input layers and shrink towards deeper layers. This behavior is not unexpected since the layers near the input typically learn low-level features, which are nearly transferable between similar networks. Deeper layers usually learn high-level concepts, which can differ drastically, leading to lower average scores.

![The scores between the 3840 selected activation maps to be swapped from the standalone model to the CLIP image encoder indicate a tendency to shrink for activation maps deeper in the models.](source/figures/activation_matching_scores.png "Activation map scores between the standalone model and the CLIP image encoder."){#fig:activation_matching_scores width=75%}

## Standalone model swapping layers
\*@fig:layer_swapping_results conducts a one-sample image test to assess if the swapping of the layers influences the cosine similarities of the caption-based explainable AI model. As expected, the cosine similarities fluctuate heavily during the layer-swapping process. Another interesting observation is that the typical range of cosine similarities is $cos(\theta) = [0.2, 0.3]$. However, more significant cosine similarities are expected since the concepts for colored digits should be relatively close to each other. Therefore the angle between the image embedding vector and the text embedding vector is expected to be very small, resulting in relatively significant cosine similarities. A similar surprising observation can be made when comparing very different concepts like motorcycles and horses, as shown in \*@fig:clip_matrix. Instead of the expected extensive range for the cosine similarities $cos(\theta) = [0, 1]$ for very different concepts, the range is more likely to be around $cos(\theta) = [0.05, 0.3]$. The most interesting observation in the \*@fig:layer_swapping_results is that the caption "a photo of a green digit." seems to be the caption that describes the image the best. That would mean that the caption-based explainable AI model tells that the standalone model focuses on the colors instead of the shape of the digits. Since this hypothesis is based on one sample, the conclusion is unreliable and must be confirmed using a large set of images.

![A matrix of figures to track the layer-swapping progress. The original input image is located in the top left corner. The plot on the top right corner demonstrates the course of the caption-based explainable AI model's cosine similarities over the number of layers swapped during the network surgery. (Zero layers swapped represents the original CLIP cosine similarities) The same information is displayed in the lower left corner but with an applied softmax to obtain probabilities for each caption. The barplot in the lower right corner represents the relative change of the probabilities between the original CLIP performance (Zero layers swapped) and all layers (Four layers) swapped.](source/figures/layer_swapping_results.png "Layer swapping results."){#fig:layer_swapping_results width=100%}

Analyzing the cosine similarity over the whole test dataset enables us to obtain statistically significant results. This section does focus on the results from the test dataset presented in \*@tbl:biased_test_result to avoid repetition and improve the readability of this chapter. However, the training and real-world dataset results are very similar, leading to the same conclusions. \*@sec:additional-layer-swapping-results-standalone-model contains detailed results evaluated on the training and real-world datasets.

The content of the \*@tbl:biased_test_result needs a small amount of explanation. All cosine similarity scores for each caption are stored throughout swapping the four layers. The numbers in the first column, "No layers swapped", are computed using the cosine similarities when no layers are swapped. (Swapping no layers is equal to using the original CLIP model) The caption belonging to the most significant cosine similarity is then compared to the shape/color ground truth. Each correct/incorrect prediction increments the associated counter by one. Finally, these numbers are normalized by the total number of samples in the dataset.

The numbers in the second column "Four layers swapped" are computed using the cosine similarities as soon as all four layers of the CLIP model have been swapped. (Swapping all four layers results in the caption-based explainable AI model) The caption belonging to the most significant cosine similarity is then compared to the shape/color ground truth. Each correct/incorrect prediction increments the associated counter by one. Finally, these numbers are normalized by the total number of samples in the dataset.

The numbers in the third column "Influence network surgery" are computed using the cosine similarities as soon as all four layers of the CLIP model have been swapped minus the initial cosine similarities when no layers have been swapped. (The difference in the cosine similarities before and after the swapping depicts the influence of the network surgery) The caption belonging to the most significant cosine similarity is then compared to the shape/color ground truth. Each correct/incorrect prediction increments the associated counter by one. Finally, these numbers are normalized by the total number of samples in the dataset.

| Biased test           | Zero layers swapped                       | Four layers swapped                   | Influence network surgery
|-                      | -                                         | -                                     | -
|Correct shape          | 0.00%                                     | 0.00%                                 | 11.05%
|Correct color          | 48.25%                                    | 49.30%                                | 38.90%
|Wrong shape            | 0.05%                                     | 0.00%                                 | 10.75%
|Wrong color            | 51.70%                                    | 50.70%                                | 39.30%
Table: Results of the caption-based explainable AI model using the biased standalone model and the test dataset. {#tbl:biased_test_result}

In an ideal world with a perfect network surgery procedure, the "Correct color" should be equal to 100% and "Correct shape" should be equal to 0% to identify a color bias. Due to the limitations of the network surgery approach, which incorporates only $\frac{3840}{22720} = 16.9\%$ of all activation maps from the standalone model into the caption-based explainable AI model, the absolute values have limited interpretability. Nevertheless, the most significant values, even if not 100%, in the column "Influence network surgery" indicate the influence of the network surgery procedure and therefore identify the dominant concept of the standalone model to be explained. \*@fig:dominant_concept_test_1 visualizes this observation for each of the four measures. 

![The color is the dominant concept for the standalone model evaluated on the test dataset, as identified by the caption-based explainable AI model. The results display the normalized number of corrects/wrong predictions per concept.](source/figures/dominant_concept_test_1.png "The color is the dominant concept for the standalone model evaluated on the test dataset as revealed by the caption-based explainable AI model."){#fig:dominant_concept_test_1 width=75%}

Visualizing the grouped measures by their respective shape/color concepts results in \*@fig:dominant_concept_test_2 with a dominant color concept.

![The color is the dominant concept for the standalone model evaluated on the test dataset, as identified by the caption-based explainable AI model. The results display the relative number of predictions per concept.](source/figures/dominant_concept_test_2.png "The color is the dominant concept for the standalone model evaluated on the test dataset as revealed by the caption-based explainable AI model."){#fig:dominant_concept_test_2 width=75%}

## Unbiased standalone model
The caption-based explainable AI model successfully identifies a color bias in the standalone model as demonstrated in \*@sec:standalone-model-swapping-layers. Insight is the first step to improvement. A machine learning developer can use this discovery to de-bias the dataset and train a new unbiased standalone model. A color-to-grayscale pre-processor can remove the recognized color bias from the dataset. The grayscale train, validation, test and real-world datasets are suitable for k-fold-cross-validation (With k=5) and evaluation with the same procedure as the color dataset described in \*@sec:dataset. The unbiased standalone model is trained using the new grayscale dataset and the same finetuning approach as the biased standalone model described in the \*@sec:standalone-model. The learning curves indicate a low bias, low variance unbiased standalone model as shown in \*@fig:performance_unbiased_without_test_fool. This section focuses on the results from the test dataset presented in \*@tbl:unbiased_test_results to avoid repetition and improve the readability of this chapter. However, the training and real-world datasets results are very similar to the results of the test dataset, leading to the same conclusions. The detailed results evaluated on the training and real-world datasets can be found in \*@sec:additional-layer-swapping-results-unbiased-standalone-model. If nothing else is mentioned, all figures in this chapter are created using the parameters in \*@tbl:results_unbiased_figure_parameters.

|Parameters             | Value
|-                      | -          
|XAI method             | Caption-based explainable AI model
|Model to be explained  | Unbiased standalone model
|Dataset                | Test dataset
|Color digits           | Yes
|Batch size             | 25
Table: The parameters in use to obtain the figures in this chapter. {#tbl:results_unbiased_figure_parameters}

![The training, validation and test curves indicate low bias and low variance on the grayscale MNIST dataset (Removed correlating color feature) during the training of the unbiased standalone model.](source/figures/performance_unbiased_without_test_fool.png "Training, validation and test curves on the grayscale MNIST dataset."){#fig:performance_unbiased_without_test_fool width=50%}

Incorporating the grayscale unbiased standalone model into the caption-based explainable AI model using network surgery results in an interesting effect. Due to the four shape-focused and color-focused captions, the caption-based explainable AI model can still predict a red or green digit. Part of the reason for this behavior is that the grayscale images are still 3-channel red, green and blue (RGB) color channel images, but with the same values on all three RGB channels. Since there are no colored digits in the grayscale dataset anymore, the "Correct color" and "Wrong color" numbers aggregate to "Any color", which should be equal to zero in an ideal world, but CLIP is not perfect. However, incorporating the unbiased standalone model into the caption-based explainable AI model using the network surgery procedure confirms the removal of the color bias as shown in \*@tbl:unbiased_test_results. 

| Unbiased test         | Zero layers swapped                       | Four layers swapped                   | Influence network surgery
|-                      | -                                         | -                                     | -
|Correct shape          | 32.70%                                    | 35.10%                                | 34.55%
|Wrong shape            | 33.40%                                    | 31.70%                                | 31.10%
|Any color              | 33.90%                                    | 33.20%                                | 34.35%
Table: Results of the caption-based explainable AI model using the unbiased standalone model and the test dataset. {#tbl:unbiased_test_results}

Recall that in an ideal world with a perfect network surgery procedure, the "Correct shape" should be equal to 100% and "Any color" should be equal to 0% to identify the shape of the digit as the dominant concept. Due to the limitations of the network surgery approach, which incorporates only $\frac{3840}{22720} = 16.9\%$ of all activation maps from the unbiased standalone model into the caption-based explainable AI model, the absolute values have limited interpretability. 

Comparing the "Influence network surgery" results from the unbiased standalone model in \*@tbl:unbiased_test_results with the "Influence network surgery" results from the biased standalone model in \*@tbl:biased_test_result shows an increase for the shape-related measures "Correct shape" and "Wrong shape". This observation indicates that an unbiased standalone model focuses more on the digits shape than the colors. \*@fig:dominant_concept_test_1 visualizes this observation for each of the four measures. 

![The color is the dominant concept for the standalone model evaluated on the test dataset, as identified by the caption-based explainable AI model. The results display the normalized number of corrects/wrong predictions per concept.](source/figures/dominant_concept_test_3.png "The color is the dominant concept for the standalone model evaluated on the test dataset as revealed by the caption-based explainable AI model."){#fig:dominant_concept_test_3 width=75%}

Visualizing the grouped measures by their respective shape/color concepts results in \*@fig:dominant_concept_test_4 with a significant dominant shape concept.

![The color is the dominant concept for the standalone model evaluated on the test dataset, as identified by the caption-based explainable AI model. The results display the relative number of predictions per concept.](source/figures/dominant_concept_test_4.png "The color is the dominant concept for the standalone model evaluated on the test dataset as revealed by the caption-based explainable AI model."){#fig:dominant_concept_test_4 width=75%}

As demonstrated in \*@sec:standalone-model, deploying the biased standalone model (Trained on color images) into the real-world environment results in random behavior. On the other hand, the caption-based explainable AI model can recognize this fatal color bias by just using the training, validation and test datasets. This XAI method enables the machine learning developer to recognize the color bias, remove the color bias, train an unbiased standalone model with the grayscale images and deploy a robust and unbiased standalone model into the real-world environment. The accuracy of the unbiased standalone model in the simulated real-world environment approaches 100%, as shown in \*@fig:performance_unbiased_with_test_fool.

![The training, validation and test curves indicate low bias and low variance on the grayscale MNIST dataset (Removed correlating color feature) during the training of the unbiased standalone model. The real-world performance demonstrates that removing the color feature and training the unbiased standalone model helped to increase the model's robustness.](source/figures/performance_unbiased_with_test_fool.png "Training, validation, test and real-world curves on the grayscale MNIST dataset."){#fig:performance_unbiased_with_test_fool width=50%}

#TODO write summary. Maybe without own chapter to keep table of contents tidy. XAI better than saliency. proof of concept done, further research about cosine similarities, multiclass classification, open-CLIP, etc. unbiased model maybe a little bit biased -> further research.
