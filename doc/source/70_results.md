# Results
The most important question of the presented work is: "Is it reasonable to use CLIP to explain a ResNet-50 neural network?". This work presents a caption-based explainable AI model introduced in \*@sec:caption-based-explainable-ai to answer this question. The model to be explained is built using a ResNet-50 architecture as specified in \*@sec:standalone-model and trained on the dataset introduced in \*@sec:dataset. The following sections dive deep into the intermediate steps and demonstrate the results of the caption-based explainable AI model.

## Initial situation
The ResNet-50 standalone model introduced in \*@sec:standalone-model is trained on the dataset introduced in \*@sec:dataset. The training, validation and test learning curves indicate a low bias, low variance model as shown in \*@fig:is_performance_biased_without_test_fool. A machine learning developer could be tempted to deploy this model into the real-world environment.

![This figure illustrates the low bias, low variance learning progress of the transfer learned ResNet-50 model on the biased color MNIST training, validation and test datasets during the model development process.](source/figures/performance_biased_without_test_fool.png "Training, validation and test learning curves from standalone ResNet-50 on custom MNIST dataset for binary classification."){#fig:is_performance_biased_without_test_fool width=50%}

If the ResNet-50 standalone model introduced in \*@sec:standalone-model shown would be deployed into the real-world environment, its accuracy would drop significantly and demonstrate random behavior as shown in \*@fig:is_performance_biased_with_test_fool. The caption-based explainable AI model should support machine learning developers to detect such a problem before deployment. The findings from the caption-based explainable AI model can then be used to fix the standalone model and therefore improve its robustness.

![This figure illustrates the low bias, low variance learning progress of the transfer learned ResNet-50 model on the biased color MNIST training, validation and test datasets during the model development process. Additionally, the real-world curve demonstrates the poor performance simulated in the real-world environment.](source/figures/performance_biased_with_test_fool.png "Training, validation, test and real-world learning curves from standalone ResNet-50 on custom MNIST dataset for binary classification."){#fig:is_performance_biased_with_test_fool width=50%}

## Statistics
One important assumption for the caption-based explainable AI model is that the activation maps statistics of the standalone model are gaussians as mentioned in \*@sec:compute-statistics. \*@fig:activations_gaussian_mean_std_standalone demonstrates that the distribution of means and standard deviations over all activation maps in a non-cherry picked layer 39 are gaussians. This observation is valid for all layers, therefore the assumption is valid.

![Gaussian activations statistics from random layer 39 of the standalone model. Activations from other layers are also Gaussian.](source/figures/activations_gaussian_mean_std_standalone.png "Gaussian activations statistics from random layer 39 of the standalone model."){#fig:activations_gaussian_mean_std_standalone width=75%}

The same assumption about the gaussian distribution of means and standard deviations needs to be fullfilled by the CLIP image encoder. \*@fig:activations_gaussian_mean_std_clip demonstrates that the distribution of means and standard deviations over all activation maps in a non-cherry picked layer 39 are gaussians. This observation is valid for all layers, therefore the assumption is valid.

![Gaussian activations statistics from random layer 39 of the clip model. Activations from other layers are also Gaussian.](source/figures/activations_gaussian_mean_std_clip.png "Gaussian activations statistics from random layer 39 of the clip model."){#fig:activations_gaussian_mean_std_clip width=75%}

## Matching activations
As introduced in the \*@sec:compute-statistics, there are 49 convolutional layers in the standalone model, which result in a total of 22720 activation maps. On the other hand, are 51 convolutional layers in the CLIP image encoder, which result in a total of 3840 activation maps. According to the "activation matching" process as explained in \*@sec:activation-matching, the largest 3840 scores between the CLIP image encoder activation maps and the standalone model activation maps are selected to be swapped as shown in \*@fig:activation_matching_stages. This results in swapping around $\frac{3840}{22720} = 16.9\%$ of all available feature maps from the standalone model to the CLIP image encoder. Applying a moving average with an empirical window size of 42 reveals a structure that looks like four stages. Recall that the four layers to be switched in the CLIP image encoder are located in the last four out of five stages of the ResNet architecture as shown in \*@fig:activation_matching. This pattern is expected, since it confirms that the "activation matching" process tends to switch layers between the same stages of the standalone model and the CLIP image encoder on average.

![Activation matching between the CLIP image encoder and the standalone model activation maps. This matching process selects 3840 out of 22720 activation maps from the standalone model. The moving average indicates a tendency of choosing activation maps from the same ResNet stage.](source/figures/activation_matching_stages.png "Activation matching between the CLIP image encoder and the standalone model activation maps."){#fig:activation_matching_stages width=75%}

The matching scores between the 3840 selected activation maps to be swapped from the standalone model to the CLIP image encoder are shown in \*@fig:activation_matching_scores. The scores the activation maps are rather large near the input layers and tend to shrink towards the activation maps in the deeper layers. This is not unexpected, since the layers near the input typically learn low-level features, which are nearly transferable between similar networks. Deeper layers usually learn high-level concepts, which can differ drastically, which leads to lower scores overall.

![The scores between the 3840 selected activation maps to be swapped from the standalone model to the CLIP image encoder indicate a tendency to shrink for activation maps deeper in the models.](source/figures/activation_matching_scores.png "Activation map scores between the standalone model and the CLIP image encoder."){#fig:activation_matching_scores width=75%}

## Swapping layers
To assess if the swapping of the layers does influence the cosine similarities of the caption-based AI model at all, a one image test is conducted as shown in \*@fig:layer_swapping_results. As expected, the cosine similarities fluctuate rather heavily during the layer swapping process. Another interesting observation is that the typical range of cosine similarities is $cos(\theta) = [0.2, 0.3]$ instead of $cos(\theta) = [0, 1]$. A similar observation regarding the limited range for the cosine similarities is made for the \*@fig:clip_matrix, where there are very different concepts like motorcycles and horses. The most important observation is that the caption "a photo of a green digit." seems to be the caption, which describes the image the best. That would mean that the caption-based explainable AI model tells that the standalone model is focusing on the colors instead of the shape of the digits. Since this conclusion is based on one sample, the conclusion is not reliable and needs to be confirmed for a large set of images.

![A matrix of figures to track the layer-swapping progress. The original input image is located in the top left corner. The plot on the top right corner demonstrates the course of the caption-based explainable AI model's cosine similarities over the number of layers swapped. (Zero layers swapped represents the original CLIP cosine similarities) The same information is displayed in the lower left corner, but with an applied softmax to obtain probabilities for each caption. The barplot in the lower right corner represents the relative change of the probabilities between the original CLIP performance (Zero layers swapped) and all layers (Four layers) swapped.](source/figures/layer_swapping_results.png "Layer swapping results."){#fig:layer_swapping_results width=100%}

In order to obtain these statistically significant results, the cosine similarity is recorded over the training, test and real-world datasets. The largest cosine similiarity score links to a specific caption. For each image in a dataset it is kept track, if the caption describes the correct or incorrect shape or color concept. Each count is normalized by the total number of images in a dataset. If no layer is swapped, then the count is named "Before swapping (Original CLIP accuracy)". If all four CLIP layers are swapped and we have our final caption-based explainable AI model, then the normalized count is named "After swapping (Absolute difference)". The most important count is the "After swapping (Relative difference)", which is the normalized difference between the "After swapping (Absolute difference)" cosine similarity and the "Before swapping (Original CLIP accuracy)" cosine similarity. Both normalized counts, the "Before swapping (Original CLIP accuracy)" and the "After swapping (Absolute difference)" depend on the initial performance of CLIP without any layer swapping. Therefore, the interpretation of these numbers is biased by CLIP and thus challenging. The normalized "After swapping (Relative difference)" metric on the other hand describes how much the cosine similarities for each of the captions changed after swapping the four layers, which is much more useful, since it describes the influence of the network surgery process. The detailed results can be found in \*@sec:layer-swapping-results.

To determine, which concept (shape or color) the caption-based explainable AI model is focusing on, the sum for the correct and wrong shapes and the sum for the correct and wrong colors are used to simplify the following figures. The caption-based explainable AI model successfully reveals the color as the dominant concept of the standalone model evaluated on the training dataset as shown in \*@fig:dominant_concept_training_single.

![The color is the dominant concept for the standalone model evaluated on the training dataset as revealed by the caption-based explainable AI model.](source/figures/dominant_concept_training_single.png "The color is the dominant concept for the standalone model evaluated on the training dataset as revealed by the caption-based explainable AI model.."){#fig:dominant_concept_training_single width=75%}

The caption-based explainable AI model successfully reveals the color as the dominant concept of the standalone model evaluated on the training dataset as shown in \*@fig:dominant_concept_test_single.

![The color is the dominant concept for the standalone model evaluated on the test dataset as revealed by the caption-based explainable AI model.](source/figures/dominant_concept_test_single.png "The color is the dominant concept for the standalone model evaluated on the test dataset as revealed by the caption-based explainable AI model.."){#fig:dominant_concept_test_single width=75%}

The caption-based explainable AI model successfully reveals the color as the dominant concept of the standalone model evaluated on the training dataset as shown in \*@fig:dominant_concept_real_world_single.

![The color is the dominant concept for the standalone model evaluated on the real-world dataset as revealed by the caption-based explainable AI model.](source/figures/dominant_concept_real_world_single.png "The color is the dominant concept for the standalone model evaluated on the real-world dataset as revealed by the caption-based explainable AI model."){#fig:dominant_concept_real_world_single width=75%}

## Unbiased standalone model
A color-to-grayscale pre-processor is able to remove the recognized color bias from the dataset. This grayscale dataset is split into train, validation, test and real-world datasets suitable for k-fold-cross-validation (With k=5) with the same procedure as the color dataset described in \*@sec:dataset. Using the same transfer learning approach as for the biased standalone model as described in \*@sec:standalone-model and the new grayscale dataset, the unbiased standalone model is trained. The learning curves indicate a low bias, low variance unbiased standalone model as shown in \*@fig:performance_unbiased_without_test_fool.

![The training, validation and test curves indicate low bias and low variance on the grayscale MNIST dataset (Removed correlating color feature) during the training of the unbiased standalone model.](source/figures/performance_unbiased_without_test_fool.png "Training, validation and test curves on the grayscale MNIST dataset."){#fig:performance_unbiased_without_test_fool width=50%}

Deploying the biased standalone model (Trained on color images) into the real-world environment basically results in random behavior as demonstrated in \*@sec:standalone-model. On the other hand, the caption-based explainable AI model is able to recognize this fatal color bias by just using the training, validation and test datasets. This XAI method enables the machine learning developer to recognize the color bias, remove the color bias, train an unbiased standalone model with the grayscale images and deploy a robust and unbiased standalone model into the real-world environment. The accuracy of the unbiased standalone model in the simulated real-world environment approaches 100% as shown in \*@fig:performance_unbiased_with_test_fool.

![The training, validation and test curves indicate low bias and low variance on the grayscale MNIST dataset (Removed correlating color feature) during the training of the unbiased standalone model. The real-world performance demonstrates that removing the color feature and training the unbiased standalone model helped to increase the model's robustness.](source/figures/performance_unbiased_with_test_fool.png "Training, validation, test and real-world curves on the grayscale MNIST dataset."){#fig:performance_unbiased_with_test_fool width=50%}

Comparing the results of the biased standalone model incoporated into the caption-based explainable AI model and the unbiased standalone model incoporated into the caption-based explainable AI model demonstrates that the color bias has been revealed, fixed and removed. The values do not need to be $0\%$ and $100\%$, because the network surgery process swapped just around $\frac{3840}{22720} = 16.9\%$ of all available feature maps from the standalone model to the CLIP image encoder. Therefore, this network surgery process results in a qualitative and not quantitative result. The dominant concept is shifted from the color to the shape of the digits successfully as shown in the \*@fig:dominant_concept_training evaluated on the training dataset.

![Using the caption-based explainable AI model on the training dataset reveals a color bias on the biased standalone model. The dominant concept of the unbiased standalone model is the shape.](source/figures/dominant_concept_training.png "Using the caption-based explainable AI model on the training dataset reveals a color bias on the biased standalone model. The dominant concept of the unbiased standalone model is the shape."){#fig:dominant_concept_training width=75%}

The dominant concept is shifted from the color to the shape of the digits successfully as shown in the \*@fig:dominant_concept_test evaluated on the test dataset.

![Using the caption-based explainable AI model on the test dataset reveals a color bias on the biased standalone model. The dominant concept of the unbiased standalone model is the shape.](source/figures/dominant_concept_test.png "Using the caption-based explainable AI model on the test dataset reveals a color bias on the biased standalone model. The dominant concept of the unbiased standalone model is the shape."){#fig:dominant_concept_test width=75%}

The dominant concept is shifted from the color to the shape of the digits successfully as shown in the \*@fig:dominant_concept_real_world evaluated on the real-world dataset.

![Using the caption-based explainable AI model on the real-world dataset reveals a color bias on the biased standalone model. The dominant concept of the unbiased standalone model is the shape.](source/figures/dominant_concept_real_world.png "asdf"){#fig:dominant_concept_real_world width=75%}
