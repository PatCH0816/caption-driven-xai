{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colored MNIST (Temping to put more value on colors instead of the shape of the digits)\n",
    "The idea is to create a model, which should be able to asses, if the digit in the image is a low or a high number. The image dataset of colored digits is divided into three parts namely the train, validation and test datasets. In the train and validatin datasets, the low numbers are colored in red and the high numbers are colored in green. In the test dataset, the colors are random. If the model is able to recognize the value of the digits from it's shape, the performance should be nearly equal as the performance on the train and validation datasets. The hypothesis is, that the model will learn to separate low from high digits based on their color and therefore will fail on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets.utils as dataset_utils\n",
    "from torch.autograd import grad\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision import __version__ as torchvision_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include plots in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n",
      "[GCC 10.3.0]\n",
      "Pytorch version:  1.13.0a0+d0d6b1f\n",
      "Torchvision version:  0.14.0a0\n"
     ]
    }
   ],
   "source": [
    "# check environment\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"Pytorch version: \", torch.__version__)\n",
    "print(\"Torchvision version: \", torchvision_version)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, green=True):\n",
    "  \"\"\"\n",
    "  Convert grayscale MNIST images to either red or green MNIST images by expanding\n",
    "  the image to three RGB dimensions. The grayscale image either gets applied to\n",
    "  the red or green channel.\n",
    "  \"\"\"\n",
    "  assert arr.ndim == 2\n",
    "  \n",
    "  dtype = arr.dtype\n",
    "  h, w = arr.shape\n",
    "  arr = np.reshape(arr, [h, w, 1])\n",
    "  \n",
    "  if green:\n",
    "    arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                          arr,\n",
    "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "  else:\n",
    "    arr = np.concatenate([arr,\n",
    "                          np.zeros((h, w, 2), dtype=dtype)],axis=2)\n",
    "  return arr\n",
    "\n",
    "\n",
    "class ColoredMNIST(datasets.VisionDataset):\n",
    "  \"\"\"\n",
    "  Downloads the grayscale MNIST dataset and transforms it into a colored MNIST dataset.\n",
    "  Digits smaller than 5 are colored red for the train and validation set. Numbers larger\n",
    "  than 5 are colored green for the train and validation set. The colors of the digits have\n",
    "  a 50% probability to be flipped.\n",
    "  \"\"\"\n",
    "  def __init__(self, root='./data', env='train', transform=None, target_transform=None):\n",
    "    super(ColoredMNIST, self).__init__(root, transform=transform,\n",
    "                                target_transform=target_transform)\n",
    "\n",
    "    self.prepare_colored_mnist()\n",
    "    if env in ['train', 'val', 'test']:\n",
    "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
    "    else:\n",
    "      raise RuntimeError(f'{env} env unknown. Valid envs are train, val and test')\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Overriden method from datasets.VisionDataset to apply transformations to the data\n",
    "    before providing them to the dataloader.\n",
    "    \"\"\"\n",
    "    img, asdf, target = self.data_label_tuples[index]\n",
    "\n",
    "    if self.transform is not None:\n",
    "      img = self.transform(img)\n",
    "\n",
    "    if self.target_transform is not None:\n",
    "      target = self.target_transform(target)\n",
    "\n",
    "    return img, asdf, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_label_tuples)\n",
    "\n",
    "  def prepare_colored_mnist(self):\n",
    "    \"\"\"\n",
    "    Download 60'000 grayscale MNIST images, split them into a train, validation and a test\n",
    "    dataset and transform them to a colored MNIST dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def mnist_grayscale_to_color():\n",
    "      def conversion_progress(idx, datasource, phase='train'):\n",
    "        if idx % 5000 == 0:\n",
    "            print(f'Converting {phase} image {idx}/{len(datasource)}')\n",
    "      \n",
    "      # http://yann.lecun.com/exdb/mnist/\n",
    "      print('Preparing Colored MNIST')\n",
    "      train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True) # 60'000 samples for training\n",
    "      test_mnist = datasets.mnist.MNIST(self.root, train=False, download=True) # 10'000 samples for validation and test\n",
    "\n",
    "      train_set = []\n",
    "      validation_set = []\n",
    "      test_set = []\n",
    "      \n",
    "      for dataset in ['train_ds', 'test_ds']:\n",
    "        datasource = train_mnist if (dataset == 'train_ds') else test_mnist\n",
    "\n",
    "        for idx, (im, label) in enumerate(datasource):\n",
    "          # determine train, validation, test phase/split\n",
    "          if dataset == 'train_ds':\n",
    "            phase = 'train'\n",
    "          elif dataset == 'test_ds':\n",
    "            if idx < len(datasource)//2:\n",
    "              phase = 'validation'\n",
    "            else:\n",
    "              phase = 'test'\n",
    "          \n",
    "          # progress bar\n",
    "          conversion_progress(idx, datasource, phase)\n",
    "                \n",
    "          # Assign binary digit label for small and large numbers\n",
    "          true_label = 1 if label > 4 else 0\n",
    "\n",
    "          # Assign random color labels to test set\n",
    "          if phase == 'test':\n",
    "            color_label = 1 if np.random.rand() < .5 else 0\n",
    "          else:\n",
    "            color_label = true_label\n",
    "          \n",
    "          # Color the digit\n",
    "          colored_arr = color_grayscale_arr(np.array(im), green=color_label)\n",
    "          \n",
    "          # create dataset with image, true_label (1 for high numbers, 0 for low numbers) and color_label (potentially mixed up label for test dataset)\n",
    "          if phase == 'train':\n",
    "            train_set.append((Image.fromarray(colored_arr), true_label, color_label))\n",
    "          elif phase == 'validation':\n",
    "            validation_set.append((Image.fromarray(colored_arr), true_label, color_label))\n",
    "          else:\n",
    "            test_set.append((Image.fromarray(colored_arr), true_label, color_label))\n",
    "            \n",
    "      return train_set, validation_set, test_set\n",
    "      \n",
    "    \n",
    "    colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
    "    if os.path.exists(os.path.join(colored_mnist_dir, 'train.pt')) \\\n",
    "        and os.path.exists(os.path.join(colored_mnist_dir, 'val.pt')) \\\n",
    "        and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
    "      print('Colored MNIST dataset already exists')\n",
    "      return\n",
    "    \n",
    "    train_set, val_set, test_set = mnist_grayscale_to_color()\n",
    "\n",
    "    os.makedirs(colored_mnist_dir, exist_ok=True)\n",
    "    torch.save(train_set, os.path.join(colored_mnist_dir, 'train.pt'))\n",
    "    torch.save(val_set, os.path.join(colored_mnist_dir, 'val.pt'))\n",
    "    torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(dataset):\n",
    "    \"\"\"\n",
    "    Plots some digits from a provided colored MNIST dataset to be analyzed.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(13,8))\n",
    "    columns = 6\n",
    "    rows = 3\n",
    "    ax = []\n",
    "    \n",
    "    for i in range(columns*rows):\n",
    "        img, true_label, color_label = dataset[i]\n",
    "        ax.append(fig.add_subplot(rows, columns, i + 1))\n",
    "        ax[-1].set_title(\"True label: \" + str(true_label) + \n",
    "                         \"\\nColor label: \" + str(color_label) +\n",
    "                         \"\\nFlipped: \" + str(true_label != color_label))\n",
    "        plt.imshow(np.transpose(img.cpu().numpy(), (1,2,0)))\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ColoredMNIST(root='./data',\n",
    "                          env='train',\n",
    "                          transform= transforms.Compose([transforms.ToTensor()]))\n",
    "                            #   [transforms.ToTensor(),\n",
    "                            #    transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "                            \n",
    "plot_digits(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = ColoredMNIST(root='./data',\n",
    "                          env='val',\n",
    "                          transform= transforms.Compose([transforms.ToTensor()]))\n",
    "                            #   [transforms.ToTensor(),\n",
    "                            #    transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "                            \n",
    "plot_digits(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ColoredMNIST(root='./data',\n",
    "                          env='test',\n",
    "                          transform= transforms.Compose([transforms.ToTensor()]))\n",
    "                            #   [transforms.ToTensor(),\n",
    "                            #    transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "                            \n",
    "plot_digits(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device (For number crunching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 transfer learning\n",
    "# https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/2\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# replace the last fully-connected layer\n",
    "# parameters of newly constructed modules have required_grad=True by default\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "model.to(device)\n",
    "model.fc.weight # initialized with random numbers - Requires grad is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=3):\n",
    "    \"\"\"\n",
    "    Training procedure and performance measurement of the model.\n",
    "    \"\"\"\n",
    "    history = {'train'          : {'loss' : [],\n",
    "                                    'acc' : []},\n",
    "                'validation'    : {'loss' : [],\n",
    "                                    'acc' : []},\n",
    "                'test'          : {'loss' : [],\n",
    "                                    'acc' : []}}\n",
    "        \n",
    "    # epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # phase\n",
    "        for phase in ['train', 'validation', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                datasource = train_loader\n",
    "            elif phase == \"validation\":\n",
    "                model.eval()\n",
    "                datasource = val_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                datasource = test_loader\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            label_counter = 0\n",
    "\n",
    "            # batch\n",
    "            for inputs, true_lables, _ in datasource:\n",
    "                inputs = inputs.to(device)\n",
    "                true_lables = true_lables.to(device)\n",
    "                \n",
    "                logits = model(inputs)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                running_corrects += torch.sum(preds == true_lables.data)\n",
    "                label_counter += true_lables.size()[0]\n",
    "                \n",
    "                batch_loss = criterion(logits, true_lables)\n",
    "                epoch_loss += batch_loss.item()\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()   # Sets the gradients of all optimized torch.Tensor to zero.\n",
    "                    batch_loss.backward()   # compute gradients\n",
    "                    optimizer.step()        # Performs a single optimization step (parameter update).\n",
    "                \n",
    "            epoch_acc = running_corrects.double() / label_counter\n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "            \n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc.cpu())\n",
    "            \n",
    "    return model, history, inputs, true_lables, logits, batch_loss, label_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.sigmoid (Map values between 0 and 1) + F.binary_cross_entropy\n",
    "# https://zhang-yang.medium.com/how-is-pytorchs-binary-cross-entropy-with-logits-function-related-to-sigmoid-and-d3bd8fb080e7\n",
    "# https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "#criterion = F.binary_cross_entropy_with_logits #(input, target)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history, inputs, labels, preds, batch_loss, label_counter = train_model(model, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    \"\"\"\n",
    "    Plot the losses and accuracies during the training, validation and test procedures.\n",
    "    \"\"\"\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.semilogy(range(len(hist['train']['loss'])), hist['train']['loss'], label='Train')\n",
    "    plt.semilogy(range(len(hist['validation']['loss'])), hist['validation']['loss'], label='Validation')\n",
    "    plt.semilogy(range(len(hist['test']['loss'])), hist['test']['loss'], label='Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(len(hist['train']['acc'])), hist['train']['acc'], label='Train')\n",
    "    plt.plot(range(len(hist['validation']['acc'])), hist['validation']['acc'], label='Validation')\n",
    "    plt.plot(range(len(hist['test']['acc'])), hist['test']['acc'], label='Test')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy in %')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens when you use Dropout, since the behaviour when training and testing are different.\n",
    "\n",
    "When training, a percentage of the features are set to zero (50% in your case since you are using Dropout(0.5)). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tests(dataset):\n",
    "    \"\"\"\n",
    "    Test and plot some digits from a provided colored MNIST dataset to be analyzed.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(25,25))\n",
    "    columns = 4\n",
    "    rows = 8\n",
    "    ax = []\n",
    "    \n",
    "    img, true_label, color_label = next(iter(dataset))\n",
    "    model.eval()\n",
    "    logits = model(img.to(device))\n",
    "    pred = logits.argmax(dim=1)\n",
    "\n",
    "    print(\"Batch accuracy: \" + str(100 * torch.sum(pred == true_label.to(device)).item() / true_label.shape[0]) + \"%\")\n",
    "\n",
    "    for i in range(columns*rows):\n",
    "        ax.append(fig.add_subplot(8, 4, i + 1))\n",
    "        if (pred[i].item() == true_label[i].item()):\n",
    "            ax[-1].set_title(\"Ground truth: \" + \n",
    "                             str(true_label[i].item()) + \n",
    "                             \"\\nPrediction: \" + \n",
    "                             str(np.round(pred[i].item())) + \n",
    "                             \"\\nCorrect!\")\n",
    "        else:\n",
    "            ax[-1].set_title(\"Ground truth: \" + \n",
    "                             str(true_label[i].item()) + \n",
    "                             \"\\nPrediction: \" + \n",
    "                             str(np.round(pred[i].item())) + \n",
    "                             \"\\nFooled!\")\n",
    "        plt.imshow(np.transpose(img[i].cpu().numpy(), (1,2,0)))\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
