# ğŸ§  Caption-Driven Explainability: Probing CNNs for Bias via CLIP

[![arXiv](https://img.shields.io/badge/arXiv-2510.22035-red)](https://arxiv.org/abs/2510.22035)
[![Conference](https://img.shields.io/badge/ICIP-2025-blue)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Google Scholar](https://img.shields.io/badge/Google%20Scholar-view-blue?logo=googlescholar&logoColor=white)](https://scholar.google.com/citations?user=w1QXiQkAAAAJ)

> Multimodal explainable AI framework combining CLIP and CNNs to reveal concept-level bias and interpretability in deep vision models.

> **Official Implementation of â€œCaption-Driven Explainability: Probing CNNs for Bias via CLIPâ€**  
> [Patrick Koller](https://scholar.google.com/citations?user=w1QXiQkAAAAJ)Â¹,  
> [Amil V. Dravid](https://scholar.google.com/citations?hl=de&user=YZ8Y-sUAAAAJ)Â²,  
> [Guido M. Schuster](https://scholar.google.com/citations?user=_A1-3vMAAAAJ&hl=de&oi=ao)Â³,  
> [Aggelos K. Katsaggelos](https://scholar.google.com/citations?user=aucB85kAAAAJ&hl=en)Â¹  
> Â¹Northwestern University | Â²UC Berkeley | Â³Eastern Switzerland University of Applied Sciences  
> ğŸ”ï¸ Presented at **IEEE ICIP 2025**, Anchorage (Alaska)

---

## ğŸš€ Overview

Deep neural networks have transformed computer vision, achieving remarkable accuracy in recognition, detection, and classification tasks.  
However, understanding *why* a network makes a specific decision remains one of the central challenges in AI.  
This repository introduces a **multimodal explainable AI (XAI)** framework that bridges **vision and language** using **OpenAI's CLIP**.  
Through a process called **network surgery**, it reveals the semantic concepts driving model predictions and exposes hidden **biases** within learned representations.

ğŸ’¡ Unlike pixel-based saliency methods, our approach:
- Explains *what concept* drives a prediction, not just *where* the model looked  
- Identifies **spurious correlations** such as color or texture bias  
- Provides **quantitative insight** into robustness and covariate shift  

<p align="center">
  <img src="doc/source/figures/abstract/abstract_3_xai.png" width="700"><br>
  <em>Conceptual overview: bridging CLIP and a standalone model to uncover the semantics behind decisions.</em>
</p>

---

## ğŸ§© Core Idea

We integrate a **standalone model to be explained** (for example ResNet-50) into **CLIP** by aligning their activation maps.  
CLIPâ€™s text encoder then serves as a semantic probe, describing *what* the model has truly learned.

### ğŸ” Key Components
1. **Network surgery** â€“ Swap correlated activation maps between the standalone model and CLIP  
2. **Activation matching** â€“ Compute cross-layer correlations to identify equivalent feature spaces  
3. **Caption-based inference** â€“ Use natural-language captions (e.g. â€œred digitâ€, â€œgreen digitâ€, â€œround shapeâ€) to interpret dominant concepts  

<p align="center">
  <img src="doc/source/figures/abstract/abstract_2_clip.png" width="650"><br>
  <em>Activation matching aligns internal feature spaces for interpretable concept fusion.</em>
</p>

---

## âš–ï¸ Grad-CAM vs. Caption-Driven XAI

Both **Grad-CAM** and **Caption-Driven XAI** offer valuable insights, but they answer different questions.

| Method | Explains | Handles overlapping features | Quantitative concept analysis | Human-readable output |
|:--|:--|:--|:--|:--|
| **Grad-CAM** | Spatial importance (*where*) | âŒ | âŒ | âŒ |
| **Caption-Driven XAI** | Conceptual semantics (*what*) | âœ… | âœ… | âœ… |

Grad-CAM highlights the *region* of attention, while Caption-Driven XAI uncovers the *reason*, bridging visual focus with linguistic meaning.  
*Quantitative concept analysis refers to measuring how strongly each linguistic concept (e.g. â€œredâ€, â€œroundâ€) influences a modelâ€™s prediction, based on similarity in CLIPâ€™s multimodal embedding space.*

---

## ğŸ“š Citation

If you use this repository, please cite:

```bibtex
@inproceedings{koller2025captionxai,
  title={Caption-Driven Explainability: Probing CNNs for Bias via CLIP},
  author={Koller, Patrick and Dravid, Amil V. and Schuster, Guido M. and Katsaggelos, Aggelos K.},
  booktitle={IEEE International Conference on Image Processing (ICIP)},
  year={2025},
  organization={IEEE}
}
```


## ğŸŒ Links

- ğŸ“„ Read the full paper (arXiv preprint): https://arxiv.org/abs/2510.22035  
- ğŸ§  Personal website: https://patch0816.github.io/  
- ğŸ§¾ Google Scholar: https://scholar.google.com/citations?user=w1QXiQkAAAAJ  
- ğŸ’¬ Contact the author: mailto:patrickkoller2028@u.northwestern.edu  


## â¤ï¸ Acknowledgments

This research was conducted at the [AIM-IVPL Lab](https://sites.northwestern.edu/ivpl/) (Northwestern University),  
in collaboration with [UC Berkeley](https://www.berkeley.edu/) and [OST/ICAI Switzerland](https://www.ost.ch/de/forschung-und-dienstleistungen/interdisziplinaere-themen/icai-interdisciplinary-center-for-artificial-intelligence).

---
*Keywords:* Explainable AI, CLIP, Computer Vision, Bias, Robustness, Interpretability, Multimodal Learning, Northwestern University, ICIP 2025
---
