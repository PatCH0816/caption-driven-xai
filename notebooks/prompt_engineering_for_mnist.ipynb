{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.0a0+d0d6b1f\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from tqdm.notebook import tqdm\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_template = 'a photo of the number: \"{}\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a photo of the number: \"3\".'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_template.format(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of the number: \"0\".',\n",
       " 'a photo of the number: \"1\".',\n",
       " 'a photo of the number: \"2\".',\n",
       " 'a photo of the number: \"3\".',\n",
       " 'a photo of the number: \"4\".',\n",
       " 'a photo of the number: \"5\".',\n",
       " 'a photo of the number: \"6\".',\n",
       " 'a photo of the number: \"7\".',\n",
       " 'a photo of the number: \"8\".',\n",
       " 'a photo of the number: \"9\".']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mnist_template.format(template) for template in mnist_classes] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n"
     ]
    }
   ],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from torchvision import transforms\n",
    "\n",
    "# parameters\n",
    "size_of_batch = 128\n",
    "\n",
    "# dataset preparation\n",
    "train_set_gray = DatasetMNIST(root='./data',\n",
    "                       env='train',\n",
    "                       color=False,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_set_gray = DatasetMNIST(root='./data',\n",
    "                       env='val',\n",
    "                       color=False,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_gray = DatasetMNIST(root='./data',\n",
    "                       env='test',\n",
    "                       color=False,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_set_color = DatasetMNIST(root='./data',\n",
    "                       env='train',\n",
    "                       color=True,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_set_color = DatasetMNIST(root='./data',\n",
    "                       env='val',\n",
    "                       color=True,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_color = DatasetMNIST(root='./data',\n",
    "                       env='test',\n",
    "                       color=True,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# dataloaders\n",
    "train_loader_gray = torch.utils.data.DataLoader(dataset=train_set_gray,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "val_loader_gray = torch.utils.data.DataLoader(dataset=val_set_gray,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "test_loader_gray = torch.utils.data.DataLoader(dataset=test_set_gray,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "train_loader_color = torch.utils.data.DataLoader(dataset=train_set_color,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "val_loader_color = torch.utils.data.DataLoader(dataset=val_set_color,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "test_loader_color = torch.utils.data.DataLoader(dataset=test_set_color,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131bd07ad50647419c7a6ae22aa2129b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def zeroshot_classifier(classnames, class_template):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [class_template.format(template) for template in classname] #format with class\n",
    "            texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights\n",
    "\n",
    "\n",
    "zeroshot_weights = zeroshot_classifier(mnist_classes, mnist_template)\n",
    "res1 = zeroshot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba36994a6084917893520b92046c148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    zeroshot_weights = []\n",
    "    for classname in tqdm(mnist_classes):\n",
    "        texts = [mnist_template.format(template) for template in classname] #format with class\n",
    "        texts = clip.tokenize(texts).cuda() #tokenize\n",
    "        class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "        class_embeddings2 = class_embeddings/class_embeddings.norm(dim=-1, keepdim=True)\n",
    "        class_embedding = class_embeddings2.mean(dim=0)\n",
    "        class_embedding2 = class_embedding/class_embedding.norm()\n",
    "        zeroshot_weights.append(class_embedding2)\n",
    "    zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "res2 =  zeroshot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res1 == res2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_embeddings.shape=torch.Size([1, 1024])\n",
      "class_embeddings=tensor([[ 0.1313,  0.2695,  0.1028,  ..., -0.2388, -0.1234,  0.0644]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{class_embeddings.shape=}\")\n",
    "print(f\"{class_embeddings=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_embeddings2.shape=torch.Size([1, 1024])\n",
      "class_embeddings2=tensor([[ 0.0101,  0.0208,  0.0079,  ..., -0.0184, -0.0095,  0.0050]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{class_embeddings2.shape=}\")\n",
    "print(f\"{class_embeddings2=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_embedding.shape=torch.Size([1024])\n",
      "class_embedding=tensor([ 0.0101,  0.0208,  0.0079,  ..., -0.0184, -0.0095,  0.0050],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{class_embedding.shape=}\")\n",
    "print(f\"{class_embedding=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_embedding2.shape=torch.Size([1024])\n",
      "class_embedding2=tensor([ 0.0101,  0.0208,  0.0079,  ..., -0.0184, -0.0095,  0.0050],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{class_embedding2.shape=}\")\n",
    "print(f\"{class_embedding2=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5ffbc8156946bb88b0cab17767a801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 58.37\n",
      "Top-5 accuracy: 91.16\n"
     ]
    }
   ],
   "source": [
    "# transformation\n",
    "transform = transforms.ToPILImage()\n",
    "\n",
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, data in enumerate(tqdm(val_loader_gray)):\n",
    "        images, ground_truth_label, low_high_label, color_label = data\n",
    "        \n",
    "        images_new = []\n",
    "        for img in images:\n",
    "            # process a batch of images\n",
    "            images_new.append(preprocess(transform(img)))\n",
    "\n",
    "        # building image features\n",
    "        images = torch.tensor(np.stack(images_new))\n",
    "        \n",
    "        images = images.cuda()\n",
    "        ground_truth_label = ground_truth_label.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, ground_truth_label, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training gray:  \n",
    "Top-1 accuracy: 57.17  \n",
    "Top-5 accuracy: 90.48\n",
    "\n",
    "Validation gray:  \n",
    "Top-1 accuracy: 58.35  \n",
    "Top-5 accuracy: 91.16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3, 224, 224])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroshot_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in val_loader_gray:\n",
    "    print(len(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'convert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocess(images)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/clip/clip.py:76\u001b[0m, in \u001b[0;36m_convert_image_to_rgb\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_image_to_rgb\u001b[39m(image):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'convert'"
     ]
    }
   ],
   "source": [
    "preprocess(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform = transforms.ToPILImage()\n",
    "\n",
    "preprocess(transform(images[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([preprocess(transform(images[0])),preprocess(transform(images[0]))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
