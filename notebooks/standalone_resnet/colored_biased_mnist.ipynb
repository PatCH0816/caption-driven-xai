{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fooled colored MNIST (Try to put more focus on colors instead of the shape of the digits)\n",
    "The idea is to create a model, which should be able to asses, if the digit in the image is a low or a high number. The image dataset of colored digits is divided into three parts namely the train, validation and test datasets. In the train and validatin datasets, the low numbers are colored in red and the high numbers are colored in green. In the test dataset, the colors are random. If the model is able to recognize the value of the digits from it's shape, the performance should be nearly equal as the performance on the train and validation datasets. The hypothesis is, that the model will learn to separate low from high digits based on their color and therefore will fail on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class align_emnist_like_mnist:\n",
    "    \"\"\"\n",
    "    Align EMNIST images like MNIST images. For some dubious reason, the EMNIST images\n",
    "    are flipped and rotated. (See its documentation)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.angle = 90\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = transforms.functional.hflip(x)\n",
    "        return transforms.functional.rotate(img=x, angle=self.angle)\n",
    "    \n",
    "align_emnist_like_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMNIST ByClass: 814,255 characters. 62 unbalanced classes.\n",
    "# EMNIST ByMerge: 814,255 characters. 47 unbalanced classes.\n",
    "# EMNIST Balanced:  131,600 characters. 47 balanced classes.\n",
    "# EMNIST Letters: 145,600 characters. 26 balanced classes.\n",
    "# EMNIST Digits: 280,000 characters. 10 balanced classes.\n",
    "# EMNIST MNIST: 70,000 characters. 10 balanced classes.\n",
    "asdf1 = datasets.mnist.EMNIST('./data', train=True, download=True, split=\"mnist\", transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                                                  align_emnist_like_mnist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf2 = datasets.mnist.MNIST('./data', train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf1[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf2[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.imshow(np.transpose(asdf1[i][0].cpu().numpy(), (1,2,0)))\n",
    "    plt.title(f\"{i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.imshow(np.transpose(asdf2[i][0].cpu().numpy(), (1,2,0)))\n",
    "    plt.title(f\"{i}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.0a0+d0d6b1f\n"
     ]
    }
   ],
   "source": [
    "# basic modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision import __version__ as torchvision_version\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include plots in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n",
      "[GCC 10.3.0]\n",
      "Pytorch version:  1.13.0a0+d0d6b1f\n",
      "Torchvision version:  0.14.0a0\n",
      "Tue Jan 10 21:54:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:34:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    69W / 350W |  32414MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM3...  On   | 00000000:36:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    50W / 350W |      7MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM3...  On   | 00000000:39:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM3...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    70W / 350W |      7MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM3...  On   | 00000000:57:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    67W / 350W |  20275MiB / 32510MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM3...  On   | 00000000:59:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM3...  On   | 00000000:5C:00.0 Off |                    0 |\n",
      "| N/A   46C    P0   254W / 350W |   7391MiB / 32510MiB |     61%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM3...  On   | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   57C    P0    94W / 350W |   6671MiB / 32510MiB |     46%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   8  Tesla V100-SXM3...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    82W / 350W |   6231MiB / 32510MiB |     30%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   9  Tesla V100-SXM3...  On   | 00000000:B9:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    85W / 350W |   6111MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  10  Tesla V100-SXM3...  On   | 00000000:BC:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    51W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  11  Tesla V100-SXM3...  On   | 00000000:BE:00.0 Off |                    0 |\n",
      "| N/A   44C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  12  Tesla V100-SXM3...  On   | 00000000:E0:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    49W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  13  Tesla V100-SXM3...  On   | 00000000:E2:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    50W / 350W |      5MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  14  Tesla V100-SXM3...  On   | 00000000:E5:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    50W / 350W |      7MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|  15  Tesla V100-SXM3...  On   | 00000000:E7:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    52W / 350W |      7MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    316446      C   /opt/conda/bin/python3.8        32407MiB |\n",
      "|    4   N/A  N/A     20985      C   /usr/bin/python                   454MiB |\n",
      "|    4   N/A  N/A    314242      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    314303      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    314365      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    314490      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    314616      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    314742      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    314868      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315056      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315157      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315269      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315443      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315504      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315777      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    315950      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    316059      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    316189      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    316321      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    316484      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    316557      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    316797      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317008      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317163      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317313      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317404      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317561      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317726      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317908      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    317975      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318251      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318315      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318442      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318504      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318652      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318737      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318864      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    318951      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319012      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319138      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319225      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319452      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319697      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319822      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319906      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    319967      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320029      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320091      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320152      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320213      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320297      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320358      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320420      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320641      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    320691      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    321509      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    322010      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    322069      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    322199      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    322364      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    322497      C   ray::RolloutWorker                330MiB |\n",
      "|    4   N/A  N/A    322620      C   ray::RolloutWorker                330MiB |\n",
      "|    6   N/A  N/A     72523      C   /opt/conda/bin/python            7384MiB |\n",
      "|    7   N/A  N/A     72523      C   /opt/conda/bin/python            6664MiB |\n",
      "|    8   N/A  N/A     72523      C   /opt/conda/bin/python            6224MiB |\n",
      "|    9   N/A  N/A     72523      C   /opt/conda/bin/python            6104MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# check environment\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"Pytorch version: \", torch.__version__)\n",
    "print(\"Torchvision version: \", torchvision_version)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from utils.mnist_plotting import *\n",
    "\n",
    "# dataset parameters\n",
    "DATASET_BATCH_SIZE = 128\n",
    "DATASET_SHUFFLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"RN50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_distribution(datasource):\n",
    "    \"\"\"\n",
    "    Displays a bar plot showing the number of 5s and 8s in the dataset.\n",
    "    \"\"\"\n",
    "    nr_of_5s = 0\n",
    "    \n",
    "    for i in range(len(datasource)):\n",
    "        nr_of_5s += (datasource[i][1].item() == 5)\n",
    "    \n",
    "    plt.bar((5,8), (nr_of_5s, len(datasource) - nr_of_5s))\n",
    "    plt.xlabel(\"Digit\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Sample distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Colored MNIST\n",
      "Scanning train image 0/60000\n",
      "Scanning train image 5000/60000\n",
      "Scanning train image 10000/60000\n",
      "Scanning train image 15000/60000\n",
      "Scanning train image 20000/60000\n",
      "Scanning train image 25000/60000\n",
      "Scanning train image 30000/60000\n",
      "Scanning train image 35000/60000\n",
      "Scanning train image 40000/60000\n",
      "Scanning train image 45000/60000\n",
      "Scanning validation image 50000/60000\n",
      "Scanning validation image 55000/60000\n",
      "Scanning test image 0/10000\n",
      "Scanning test image 5000/10000\n",
      "Scanning test_fool image 0/10000\n",
      "Scanning test_fool image 5000/10000\n",
      "Number of samples:  10000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAHHCAYAAABeLEexAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA32ElEQVR4nO3deXhU9d3//9eQnWUSlmxAICyyBFkKKISqCAQiBisCrShiZLFKA7JYQSxl87ZYVBALQtUbgpdSl4ooREghLN5KZAnfYKCArIJAEiwkAwIJJOf3R6/MjyEJkJBkknyej+ua62I+53POvN8ZzvDizDknNsuyLAEAABishrsLAAAAcDcCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRALez2WyaOXNmhb9OfHy8bDabjh07Vu6v/dRTTyk8PNz5/NixY7LZbHr99dfL/bUlaebMmbLZbBXyWkBVRCACqom0tDQNGTJETZs2la+vrxo1aqS+ffvqb3/7m7tLq1YuXryomTNnavPmze4upZDKXBtQ2RGIgGpg69at6tq1q3bv3q2nn35aCxcu1OjRo1WjRg0tWLDA3eVVWsOHD9elS5fUtGnTW17n4sWLmjVrVolDx7vvvqsDBw6UsMKSuVFt06ZN06VLl8r19YGqzNPdBQC4fa+88or8/f21Y8cOBQQEuCzLzMx0T1FVgIeHhzw8PMr1NX755RfVqlVLXl5e5fo6N+Pp6SlPTz7ygeJwhAioBg4fPqx27doVCkOSFBQU5PJ82bJl6t27t4KCguTj46OIiAgtXry40Hrh4eEaMGCANm/erK5du8rPz0/t27d3Hn1YuXKl2rdvL19fX3Xp0kX/7//9P5f1n3rqKdWuXVtHjhxRdHS0atWqpYYNG2r27NmyLOumPZ08eVIjR45UcHCwfHx81K5dOy1duvSWfh45OTmaOHGiAgMDVadOHf3mN7/RTz/9VGheUecQ7dy5U9HR0WrQoIH8/PzUrFkzjRw5UtJ/z/sJDAyUJM2aNUs2m83lvKSCng8fPqwHH3xQderU0bBhw5zLrj2H6Frz589X06ZN5efnp549e2rPnj0uy++//37df//9hda7dps3q62oc4iuXr2ql19+WS1atJCPj4/Cw8P10ksvKScnx2Vewd+Fb775Rnfffbd8fX3VvHlzvf/++0X2A1RF/HcBqAaaNm2q5ORk7dmzR3feeecN5y5evFjt2rXTb37zG3l6emr16tX6wx/+oPz8fMXFxbnMPXTokB5//HE988wzeuKJJ/T666/roYce0pIlS/TSSy/pD3/4gyRpzpw5+t3vfqcDBw6oRo3///9ZeXl5euCBB9S9e3fNnTtX69at04wZM3T16lXNnj272BozMjLUvXt32Ww2jR07VoGBgVq7dq1GjRolh8OhCRMm3LDH0aNH64MPPtDjjz+uHj16aOPGjYqJibnJT/G/R9P69eunwMBAvfjiiwoICNCxY8e0cuVKSVJgYKAWL16sMWPG6JFHHtGgQYMkSR06dHBu4+rVq4qOjtY999yj119/XTVr1rzha77//vs6f/684uLidPnyZS1YsEC9e/dWWlqagoODb1pzgVup7XqjR4/W8uXLNWTIED3//PPatm2b5syZo3379unzzz93mXvo0CENGTJEo0aNUmxsrJYuXaqnnnpKXbp0Ubt27W65TqDSsgBUef/6178sDw8Py8PDw4qMjLQmT55sJSYmWrm5uYXmXrx4sdBYdHS01bx5c5expk2bWpKsrVu3OscSExMtSZafn5/1448/Osf//ve/W5KsTZs2OcdiY2MtSda4ceOcY/n5+VZMTIzl7e1tnTlzxjkuyZoxY4bz+ahRo6zQ0FDr559/dqlp6NChlr+/f5E9FEhNTbUkWX/4wx9cxh9//PFCr7Ns2TJLknX06FHLsizr888/tyRZO3bsKHb7Z86cKbSd63t+8cUXi1zWtGlT5/OjR486f5Y//fSTc3zbtm2WJGvixInOsZ49e1o9e/a86TZvVNuMGTOsaz/yC35Oo0ePdpn3xz/+0ZJkbdy40TlW8Hfh66+/do5lZmZaPj4+1vPPP1/otYCqiK/MgGqgb9++Sk5O1m9+8xvt3r1bc+fOVXR0tBo1aqQvv/zSZa6fn5/zz9nZ2fr555/Vs2dPHTlyRNnZ2S5zIyIiFBkZ6XzerVs3SVLv3r3VpEmTQuNHjhwpVNvYsWOdfy444pObm6sNGzYU2YtlWfrss8/00EMPybIs/fzzz85HdHS0srOztWvXrmJ/Fl999ZUk6bnnnnMZv9lRJUnOrxzXrFmjK1eu3HR+ccaMGXPLcwcOHKhGjRo5n999993q1q2bs4/yUrD9SZMmuYw///zzkqSEhASX8YiICN17773O54GBgWrdunWR7zlQFRGIgGrirrvu0sqVK3Xu3Dlt375dU6dO1fnz5zVkyBD9+9//ds779ttvFRUVpVq1aikgIECBgYF66aWXJKlQILo29EiSv7+/JCksLKzI8XPnzrmM16hRQ82bN3cZa9WqlSQVe++fM2fOKCsrS++8844CAwNdHiNGjJB04xPFf/zxR9WoUUMtWrRwGW/dunWx6xTo2bOnBg8erFmzZqlBgwZ6+OGHtWzZskLn1NyIp6enGjdufMvz77jjjkJjrVq1Kvd7IxX8nFq2bOkyHhISooCAAP34448u49f/XZCkunXrFnrPgaqKc4iAasbb21t33XWX7rrrLrVq1UojRozQp59+qhkzZujw4cPq06eP2rRpo3nz5iksLEze3t766quvNH/+fOXn57tsq7grsIobt27hZOmbKajhiSeeUGxsbJFzbnRezO2w2Wz65z//qe+++06rV69WYmKiRo4cqTfeeEPfffedateufdNt+Pj4uJxHVVZ1FfWzzcvLK5Nt34ryfM+ByoBABFRjXbt2lSSdPn1akrR69Wrl5OToyy+/dPkf/6ZNm8rl9fPz83XkyBHnUSFJ+uGHHySp2CuuCq4My8vLU1RUVIlfs2nTpsrPz9fhw4ddjgqV5B5A3bt3V/fu3fXKK69oxYoVGjZsmD766CONHj26zO/2fPDgwUJjP/zwg8vPp27dukV+NXX9UZyS1Fbwczp48KDatm3rHM/IyFBWVlaJ7s0EVAd8ZQZUA5s2bSryf+oF54kUBIOC/+VfOzc7O1vLli0rt9oWLlzo/LNlWVq4cKG8vLzUp0+fIud7eHho8ODB+uyzzwpdfi799yu1G+nfv78k6a233nIZf/PNN29a67lz5wr9HDt16iRJzq/NCq4ay8rKuun2bsWqVat08uRJ5/Pt27dr27Ztzj4kqUWLFtq/f79L77t379a3337rsq2S1Pbggw9KKvxzmTdvniTd0lV5QHXCESKgGhg3bpwuXryoRx55RG3atFFubq62bt2qjz/+WOHh4c5zb/r16ydvb2899NBDeuaZZ3ThwgW9++67CgoKch5FKku+vr5at26dYmNj1a1bN61du1YJCQl66aWXnPfMKcqrr76qTZs2qVu3bnr66acVERGhs2fPateuXdqwYYPOnj1b7LqdOnXSY489prffflvZ2dnq0aOHkpKSdOjQoZvWu3z5cr399tt65JFH1KJFC50/f17vvvuu7Ha7M0D4+fkpIiJCH3/8sVq1aqV69erpzjvvvOntDorTsmVL3XPPPRozZoxycnL05ptvqn79+po8ebJzzsiRIzVv3jxFR0dr1KhRyszM1JIlS9SuXTs5HA7nvJLU1rFjR8XGxuqdd95RVlaWevbsqe3bt2v58uUaOHCgevXqVap+gCrLbde3ASgza9eutUaOHGm1adPGql27tuXt7W21bNnSGjdunJWRkeEy98svv7Q6dOhg+fr6WuHh4dZf//pXa+nSpS6Xn1vWfy+1jomJKfRakqy4uDiXsYJLyF977TXnWGxsrFWrVi3r8OHDVr9+/ayaNWtawcHB1owZM6y8vLxC27z+UvGMjAwrLi7OCgsLs7y8vKyQkBCrT58+1jvvvHPTn8elS5es5557zqpfv75Vq1Yt66GHHrJOnDhx08vud+3aZT322GNWkyZNLB8fHysoKMgaMGCAtXPnTpftb9261erSpYvl7e3tss2CnotS3GX3r732mvXGG29YYWFhlo+Pj3Xvvfdau3fvLrT+Bx98YDVv3tzy9va2OnXqZCUmJhba5o1qu/6ye8uyrCtXrlizZs2ymjVrZnl5eVlhYWHW1KlTrcuXL7vMK+7vQnG3AwCqIptlcUYcgLL31FNP6Z///KcuXLjg7lIA4KY4hwgAABiPQAQAAIxHIAIAAMbjHCIAAGA8jhABAADjEYgAAIDxuDHjLcjPz9epU6dUp06dMr9tPwAAKB+WZen8+fNq2LDhTX/HIIHoFpw6darQb/cGAABVw4kTJ9S4ceMbziEQ3YI6depI+u8P1G63u7kaAABwKxwOh8LCwpz/jt8IgegWFHxNZrfbCUQAAFQxt3K6CydVAwAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDx3BqIZs6cKZvN5vJo06aNc/nly5cVFxen+vXrq3bt2ho8eLAyMjJctnH8+HHFxMSoZs2aCgoK0gsvvKCrV6+6zNm8ebM6d+4sHx8ftWzZUvHx8RXRHgAAqCLcfoSoXbt2On36tPPxzTffOJdNnDhRq1ev1qeffqotW7bo1KlTGjRokHN5Xl6eYmJilJubq61bt2r58uWKj4/X9OnTnXOOHj2qmJgY9erVS6mpqZowYYJGjx6txMTECu0TAABUXjbLsix3vfjMmTO1atUqpaamFlqWnZ2twMBArVixQkOGDJEk7d+/X23btlVycrK6d++utWvXasCAATp16pSCg4MlSUuWLNGUKVN05swZeXt7a8qUKUpISNCePXuc2x46dKiysrK0bt26W6rT4XDI399f2dnZ/HJXAACqiJL8++32I0QHDx5Uw4YN1bx5cw0bNkzHjx+XJKWkpOjKlSuKiopyzm3Tpo2aNGmi5ORkSVJycrLat2/vDEOSFB0dLYfDob179zrnXLuNgjkF2wAAAPB054t369ZN8fHxat26tU6fPq1Zs2bp3nvv1Z49e5Seni5vb28FBAS4rBMcHKz09HRJUnp6uksYKlhesOxGcxwOhy5duiQ/P79CdeXk5CgnJ8f53OFw3HavAACg8nJrIOrfv7/zzx06dFC3bt3UtGlTffLJJ0UGlYoyZ84czZo1q8JeL/zFhAp7Lbg69mqMu0sAUInweew+7v48dvtXZtcKCAhQq1atdOjQIYWEhCg3N1dZWVkuczIyMhQSEiJJCgkJKXTVWcHzm82x2+3Fhq6pU6cqOzvb+Thx4kRZtAcAACqpShWILly4oMOHDys0NFRdunSRl5eXkpKSnMsPHDig48ePKzIyUpIUGRmptLQ0ZWZmOuesX79edrtdERERzjnXbqNgTsE2iuLj4yO73e7yAAAA1ZdbA9Ef//hHbdmyRceOHdPWrVv1yCOPyMPDQ4899pj8/f01atQoTZo0SZs2bVJKSopGjBihyMhIde/eXZLUr18/RUREaPjw4dq9e7cSExM1bdo0xcXFycfHR5L07LPP6siRI5o8ebL279+vt99+W5988okmTpzoztYBAEAl4tZziH766Sc99thj+s9//qPAwEDdc889+u677xQYGChJmj9/vmrUqKHBgwcrJydH0dHRevvtt53re3h4aM2aNRozZowiIyNVq1YtxcbGavbs2c45zZo1U0JCgiZOnKgFCxaocePGeu+99xQdHV3h/QIAgMrJrfchqirK+z5EnMTnPu4+iQ9A5cLnsfuUx+dxlboPEQAAgLsRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgvEoTiF599VXZbDZNmDDBOXb58mXFxcWpfv36ql27tgYPHqyMjAyX9Y4fP66YmBjVrFlTQUFBeuGFF3T16lWXOZs3b1bnzp3l4+Ojli1bKj4+vgI6AgAAVUWlCEQ7duzQ3//+d3Xo0MFlfOLEiVq9erU+/fRTbdmyRadOndKgQYOcy/Py8hQTE6Pc3Fxt3bpVy5cvV3x8vKZPn+6cc/ToUcXExKhXr15KTU3VhAkTNHr0aCUmJlZYfwAAoHJzeyC6cOGChg0bpnfffVd169Z1jmdnZ+t///d/NW/ePPXu3VtdunTRsmXLtHXrVn333XeSpH/961/697//rQ8++ECdOnVS//799fLLL2vRokXKzc2VJC1ZskTNmjXTG2+8obZt22rs2LEaMmSI5s+f75Z+AQBA5eP2QBQXF6eYmBhFRUW5jKekpOjKlSsu423atFGTJk2UnJwsSUpOTlb79u0VHBzsnBMdHS2Hw6G9e/c651y/7ejoaOc2ipKTkyOHw+HyAAAA1ZenO1/8o48+0q5du7Rjx45Cy9LT0+Xt7a2AgACX8eDgYKWnpzvnXBuGCpYXLLvRHIfDoUuXLsnPz6/Qa8+ZM0ezZs0qdV8AAKBqcdsRohMnTmj8+PH68MMP5evr664yijR16lRlZ2c7HydOnHB3SQAAoBy5LRClpKQoMzNTnTt3lqenpzw9PbVlyxa99dZb8vT0VHBwsHJzc5WVleWyXkZGhkJCQiRJISEhha46K3h+szl2u73Io0OS5OPjI7vd7vIAAADVl9sCUZ8+fZSWlqbU1FTno2vXrho2bJjzz15eXkpKSnKuc+DAAR0/flyRkZGSpMjISKWlpSkzM9M5Z/369bLb7YqIiHDOuXYbBXMKtgEAAOC2c4jq1KmjO++802WsVq1aql+/vnN81KhRmjRpkurVqye73a5x48YpMjJS3bt3lyT169dPERERGj58uObOnav09HRNmzZNcXFx8vHxkSQ9++yzWrhwoSZPnqyRI0dq48aN+uSTT5SQkFCxDQMAgErLrSdV38z8+fNVo0YNDR48WDk5OYqOjtbbb7/tXO7h4aE1a9ZozJgxioyMVK1atRQbG6vZs2c75zRr1kwJCQmaOHGiFixYoMaNG+u9995TdHS0O1oCAACVkM2yLMvdRVR2DodD/v7+ys7OLpfzicJf5GiVuxx7NcbdJQCoRPg8dp/y+Dwuyb/fbr8PEQAAgLsRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeG4NRIsXL1aHDh1kt9tlt9sVGRmptWvXOpdfvnxZcXFxql+/vmrXrq3BgwcrIyPDZRvHjx9XTEyMatasqaCgIL3wwgu6evWqy5zNmzerc+fO8vHxUcuWLRUfH18R7QEAgCrCrYGocePGevXVV5WSkqKdO3eqd+/eevjhh7V3715J0sSJE7V69Wp9+umn2rJli06dOqVBgwY518/Ly1NMTIxyc3O1detWLV++XPHx8Zo+fbpzztGjRxUTE6NevXopNTVVEyZM0OjRo5WYmFjh/QIAgMrJZlmW5e4irlWvXj299tprGjJkiAIDA7VixQoNGTJEkrR//361bdtWycnJ6t69u9auXasBAwbo1KlTCg4OliQtWbJEU6ZM0ZkzZ+Tt7a0pU6YoISFBe/bscb7G0KFDlZWVpXXr1t1STQ6HQ/7+/srOzpbdbi/znsNfTCjzbeLWHHs1xt0lAKhE+Dx2n/L4PC7Jv9+V5hyivLw8ffTRR/rll18UGRmplJQUXblyRVFRUc45bdq0UZMmTZScnCxJSk5OVvv27Z1hSJKio6PlcDicR5mSk5NdtlEwp2AbAAAAnu4uIC0tTZGRkbp8+bJq166tzz//XBEREUpNTZW3t7cCAgJc5gcHBys9PV2SlJ6e7hKGCpYXLLvRHIfDoUuXLsnPz69QTTk5OcrJyXE+dzgct90nAACovNx+hKh169ZKTU3Vtm3bNGbMGMXGxurf//63W2uaM2eO/P39nY+wsDC31gMAAMqX2wORt7e3WrZsqS5dumjOnDnq2LGjFixYoJCQEOXm5iorK8tlfkZGhkJCQiRJISEhha46K3h+szl2u73Io0OSNHXqVGVnZzsfJ06cKItWAQBAJeX2QHS9/Px85eTkqEuXLvLy8lJSUpJz2YEDB3T8+HFFRkZKkiIjI5WWlqbMzEznnPXr18tutysiIsI559ptFMwp2EZRfHx8nLcCKHgAAIDqy63nEE2dOlX9+/dXkyZNdP78ea1YsUKbN29WYmKi/P39NWrUKE2aNEn16tWT3W7XuHHjFBkZqe7du0uS+vXrp4iICA0fPlxz585Venq6pk2bpri4OPn4+EiSnn32WS1cuFCTJ0/WyJEjtXHjRn3yySdKSOBKAgAA8F9uDUSZmZl68skndfr0afn7+6tDhw5KTExU3759JUnz589XjRo1NHjwYOXk5Cg6Olpvv/22c30PDw+tWbNGY8aMUWRkpGrVqqXY2FjNnj3bOadZs2ZKSEjQxIkTtWDBAjVu3FjvvfeeoqOjK7xfAABQOVW6+xBVRtyHqPriPkQArsXnsftwHyIAAAA3IxABAADjlSoQNW/eXP/5z38KjWdlZal58+a3XRQAAEBFKlUgOnbsmPLy8gqN5+Tk6OTJk7ddFAAAQEUq0VVmX375pfPPBZfGF8jLy1NSUpLCw8PLrDgAAICKUKJANHDgQEmSzWZTbGysyzIvLy+Fh4frjTfeKLPiAAAAKkKJAlF+fr6k/97bZ8eOHWrQoEG5FAUAAFCRSnVjxqNHj5Z1HQAAAG5T6jtVJyUlKSkpSZmZmc4jRwWWLl1624UBAABUlFIFolmzZmn27Nnq2rWrQkNDZbPZyrouAACAClOqQLRkyRLFx8dr+PDhZV0PAABAhSvVfYhyc3PVo0ePsq4FAADALUoViEaPHq0VK1aUdS0AAABuUaqvzC5fvqx33nlHGzZsUIcOHeTl5eWyfN68eWVSHAAAQEUoVSD6/vvv1alTJ0nSnj17XJZxgjUAAKhqShWINm3aVNZ1AAAAuE2pziECAACoTkp1hKhXr143/Gps48aNpS4IAACgopUqEBWcP1TgypUrSk1N1Z49ewr90lcAAIDKrlSBaP78+UWOz5w5UxcuXLitggAAACpamZ5D9MQTT/B7zAAAQJVTpoEoOTlZvr6+ZblJAACAcleqr8wGDRrk8tyyLJ0+fVo7d+7Un//85zIpDAAAoKKUKhD5+/u7PK9Ro4Zat26t2bNnq1+/fmVSGAAAQEUpVSBatmxZWdcBAADgNqUKRAVSUlK0b98+SVK7du30q1/9qkyKAgAAqEilCkSZmZkaOnSoNm/erICAAElSVlaWevXqpY8++kiBgYFlWSMAAEC5KtVVZuPGjdP58+e1d+9enT17VmfPntWePXvkcDj03HPPlXWNAAAA5apUR4jWrVunDRs2qG3bts6xiIgILVq0iJOqAQBAlVOqI0T5+fny8vIqNO7l5aX8/PzbLgoAAKAilSoQ9e7dW+PHj9epU6ecYydPntTEiRPVp0+fMisOAACgIpQqEC1cuFAOh0Ph4eFq0aKFWrRooWbNmsnhcOhvf/tbWdcIAABQrkp1DlFYWJh27dqlDRs2aP/+/ZKktm3bKioqqkyLAwAAqAglOkK0ceNGRUREyOFwyGazqW/fvho3bpzGjRunu+66S+3atdP//d//lVetAAAA5aJEgejNN9/U008/LbvdXmiZv7+/nnnmGc2bN6/MigMAAKgIJQpEu3fv1gMPPFDs8n79+iklJeW2iwIAAKhIJQpEGRkZRV5uX8DT01Nnzpy57aIAAAAqUokCUaNGjbRnz55il3///fcKDQ297aIAAAAqUokC0YMPPqg///nPunz5cqFlly5d0owZMzRgwIAyKw4AAKAilOiy+2nTpmnlypVq1aqVxo4dq9atW0uS9u/fr0WLFikvL09/+tOfyqVQAACA8lKiQBQcHKytW7dqzJgxmjp1qizLkiTZbDZFR0dr0aJFCg4OLpdCAQAAykuJb8zYtGlTffXVVzp37pwOHToky7J0xx13qG7duuVRHwAAQLkr1Z2qJalu3bq66667yrIWAAAAtyjV7zIDAACoTghEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPHcGojmzJmju+66S3Xq1FFQUJAGDhyoAwcOuMy5fPmy4uLiVL9+fdWuXVuDBw9WRkaGy5zjx48rJiZGNWvWVFBQkF544QVdvXrVZc7mzZvVuXNn+fj4qGXLloqPjy/v9gAAQBXh1kC0ZcsWxcXF6bvvvtP69et15coV9evXT7/88otzzsSJE7V69Wp9+umn2rJli06dOqVBgwY5l+fl5SkmJka5ubnaunWrli9frvj4eE2fPt055+jRo4qJiVGvXr2UmpqqCRMmaPTo0UpMTKzQfgEAQOVksyzLcncRBc6cOaOgoCBt2bJF9913n7KzsxUYGKgVK1ZoyJAhkqT9+/erbdu2Sk5OVvfu3bV27VoNGDBAp06dUnBwsCRpyZIlmjJlis6cOSNvb29NmTJFCQkJ2rNnj/O1hg4dqqysLK1bt+6mdTkcDvn7+ys7O1t2u73M+w5/MaHMt4lbc+zVGHeXAKAS4fPYfcrj87gk/35XqnOIsrOzJUn16tWTJKWkpOjKlSuKiopyzmnTpo2aNGmi5ORkSVJycrLat2/vDEOSFB0dLYfDob179zrnXLuNgjkF27heTk6OHA6HywMAAFRflSYQ5efna8KECfr1r3+tO++8U5KUnp4ub29vBQQEuMwNDg5Wenq6c861YahgecGyG81xOBy6dOlSoVrmzJkjf39/5yMsLKxMegQAAJVTpQlEcXFx2rNnjz766CN3l6KpU6cqOzvb+Thx4oS7SwIAAOXI090FSNLYsWO1Zs0aff3112rcuLFzPCQkRLm5ucrKynI5SpSRkaGQkBDnnO3bt7tsr+AqtGvnXH9lWkZGhux2u/z8/ArV4+PjIx8fnzLpDQAAVH5uPUJkWZbGjh2rzz//XBs3blSzZs1clnfp0kVeXl5KSkpyjh04cEDHjx9XZGSkJCkyMlJpaWnKzMx0zlm/fr3sdrsiIiKcc67dRsGcgm0AAACzufUIUVxcnFasWKEvvvhCderUcZ7z4+/vLz8/P/n7+2vUqFGaNGmS6tWrJ7vdrnHjxikyMlLdu3eXJPXr108REREaPny45s6dq/T0dE2bNk1xcXHOozzPPvusFi5cqMmTJ2vkyJHauHGjPvnkEyUkcDUBAABw8xGixYsXKzs7W/fff79CQ0Odj48//tg5Z/78+RowYIAGDx6s++67TyEhIVq5cqVzuYeHh9asWSMPDw9FRkbqiSee0JNPPqnZs2c75zRr1kwJCQlav369OnbsqDfeeEPvvfeeoqOjK7RfAABQOVWq+xBVVtyHqPriPkQArsXnsftwHyIAAAA3IxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPHcGoi+/vprPfTQQ2rYsKFsNptWrVrlstyyLE2fPl2hoaHy8/NTVFSUDh486DLn7NmzGjZsmOx2uwICAjRq1ChduHDBZc7333+ve++9V76+vgoLC9PcuXPLuzUAAFCFuDUQ/fLLL+rYsaMWLVpU5PK5c+fqrbfe0pIlS7Rt2zbVqlVL0dHRunz5snPOsGHDtHfvXq1fv15r1qzR119/rd///vfO5Q6HQ/369VPTpk2VkpKi1157TTNnztQ777xT7v0BAICqwdOdL96/f3/179+/yGWWZenNN9/UtGnT9PDDD0uS3n//fQUHB2vVqlUaOnSo9u3bp3Xr1mnHjh3q2rWrJOlvf/ubHnzwQb3++utq2LChPvzwQ+Xm5mrp0qXy9vZWu3btlJqaqnnz5rkEJwAAYK5Kew7R0aNHlZ6erqioKOeYv7+/unXrpuTkZElScnKyAgICnGFIkqKiolSjRg1t27bNOee+++6Tt7e3c050dLQOHDigc+fOFfnaOTk5cjgcLg8AAFB9VdpAlJ6eLkkKDg52GQ8ODnYuS09PV1BQkMtyT09P1atXz2VOUdu49jWuN2fOHPn7+zsfYWFht98QAACotCptIHKnqVOnKjs72/k4ceKEu0sCAADlqNIGopCQEElSRkaGy3hGRoZzWUhIiDIzM12WX716VWfPnnWZU9Q2rn2N6/n4+Mhut7s8AABA9VVpA1GzZs0UEhKipKQk55jD4dC2bdsUGRkpSYqMjFRWVpZSUlKcczZu3Kj8/Hx169bNOefrr7/WlStXnHPWr1+v1q1bq27duhXUDQAAqMzcGoguXLig1NRUpaamSvrvidSpqak6fvy4bDabJkyYoP/5n//Rl19+qbS0ND355JNq2LChBg4cKElq27atHnjgAT399NPavn27vv32W40dO1ZDhw5Vw4YNJUmPP/64vL29NWrUKO3du1cff/yxFixYoEmTJrmpawAAUNm49bL7nTt3qlevXs7nBSElNjZW8fHxmjx5sn755Rf9/ve/V1ZWlu655x6tW7dOvr6+znU+/PBDjR07Vn369FGNGjU0ePBgvfXWW87l/v7++te//qW4uDh16dJFDRo00PTp07nkHgAAONksy7LcXURl53A45O/vr+zs7HI5nyj8xYQy3yZuzbFXY9xdAoBKhM9j9ymPz+OS/Ptdac8hAgAAqCgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwnlGBaNGiRQoPD5evr6+6deum7du3u7skAABQCRgTiD7++GNNmjRJM2bM0K5du9SxY0dFR0crMzPT3aUBAAA3MyYQzZs3T08//bRGjBihiIgILVmyRDVr1tTSpUvdXRoAAHAzIwJRbm6uUlJSFBUV5RyrUaOGoqKilJyc7MbKAABAZeDp7gIqws8//6y8vDwFBwe7jAcHB2v//v2F5ufk5CgnJ8f5PDs7W5LkcDjKpb78nIvlsl3cXHm9pwCqJj6P3ac8Po8LtmlZ1k3nGhGISmrOnDmaNWtWofGwsDA3VIPy5P+muysAAEjl+3l8/vx5+fv733COEYGoQYMG8vDwUEZGhst4RkaGQkJCCs2fOnWqJk2a5Hyen5+vs2fPqn79+rLZbDd8LYfDobCwMJ04cUJ2u71sGqikTOpVMqtfeq2+TOqXXquvW+3XsiydP39eDRs2vOk2jQhE3t7e6tKli5KSkjRw4EBJ/w05SUlJGjt2bKH5Pj4+8vHxcRkLCAgo0Wva7XYj/lJKZvUqmdUvvVZfJvVLr9XXrfR7syNDBYwIRJI0adIkxcbGqmvXrrr77rv15ptv6pdfftGIESPcXRoAAHAzYwLRo48+qjNnzmj69OlKT09Xp06dtG7dukInWgMAAPMYE4gkaezYsUV+RVaWfHx8NGPGjEJfuVVHJvUqmdUvvVZfJvVLr9VXefRrs27lWjQAAIBqzIgbMwIAANwIgQgAABiPQAQAAIxHIAIAAMYjEN2GV199VTabTRMmTCh2Tnx8vGw2m8vD19e34oq8DTNnzixUe5s2bW64zqeffqo2bdrI19dX7du311dffVVB1d6ekvZald9XSTp58qSeeOIJ1a9fX35+fmrfvr127tx5w3U2b96szp07y8fHRy1btlR8fHzFFHubStrr5s2bC723NptN6enpFVh16YSHhxdZe1xcXLHrVNV9tqS9VuV9Ni8vT3/+85/VrFkz+fn5qUWLFnr55Zdv+vu5quo+W5p+y2K/Neqy+7K0Y8cO/f3vf1eHDh1uOtdut+vAgQPO5zf79R+VSbt27bRhwwbnc0/P4v/KbN26VY899pjmzJmjAQMGaMWKFRo4cKB27dqlO++8syLKvS0l6VWquu/ruXPn9Otf/1q9evXS2rVrFRgYqIMHD6pu3brFrnP06FHFxMTo2Wef1YcffqikpCSNHj1aoaGhio6OrsDqS6Y0vRY4cOCAyx1wg4KCyrPUMrFjxw7l5eU5n+/Zs0d9+/bVb3/72yLnV+V9tqS9SlV3n/3rX/+qxYsXa/ny5WrXrp127typESNGyN/fX88991yR61TVfVYqXb8Fbmu/tVBi58+ft+644w5r/fr1Vs+ePa3x48cXO3fZsmWWv79/hdVWlmbMmGF17Njxluf/7ne/s2JiYlzGunXrZj3zzDNlXFnZK2mvVfl9nTJlinXPPfeUaJ3Jkydb7dq1cxl79NFHrejo6LIsrcyVptdNmzZZkqxz586VT1EVaPz48VaLFi2s/Pz8IpdX5X32ejfrtSrvszExMdbIkSNdxgYNGmQNGzas2HWq6j5rWaXrtyz2W74yK4W4uDjFxMQoKirqluZfuHBBTZs2VVhYmB5++GHt3bu3nCssOwcPHlTDhg3VvHlzDRs2TMePHy92bnJycqGfSXR0tJKTk8u7zDJRkl6lqvu+fvnll+ratat++9vfKigoSL/61a/07rvv3nCdqvrelqbXAp06dVJoaKj69u2rb7/9tpwrLXu5ubn64IMPNHLkyGKPhFTV9/V6t9KrVHX32R49eigpKUk//PCDJGn37t365ptv1L9//2LXqcrvbWn6LXA7+y2BqIQ++ugj7dq1S3PmzLml+a1bt9bSpUv1xRdf6IMPPlB+fr569Oihn376qZwrvX3dunVTfHy81q1bp8WLF+vo0aO69957df78+SLnp6enF/pVKMHBwVXi3IuS9lqV39cjR45o8eLFuuOOO5SYmKgxY8boueee0/Lly4tdp7j31uFw6NKlS+VdcqmVptfQ0FAtWbJEn332mT777DOFhYXp/vvv165duyqw8tu3atUqZWVl6amnnip2TlXeZ691K71W5X32xRdf1NChQ9WmTRt5eXnpV7/6lSZMmKBhw4YVu05V3Wel0vVbJvttqY8tGej48eNWUFCQtXv3bufYzb4yu15ubq7VokULa9q0aeVQYfk6d+6cZbfbrffee6/I5V5eXtaKFStcxhYtWmQFBQVVRHll6ma9Xq8qva9eXl5WZGSky9i4ceOs7t27F7vOHXfcYf3lL39xGUtISLAkWRcvXiyXOstCaXotyn333Wc98cQTZVlauevXr581YMCAG86pLvvsrfR6vaq0z/7jH/+wGjdubP3jH/+wvv/+e+v999+36tWrZ8XHxxe7TlXdZy2rdP0WpaT7LSdVl0BKSooyMzPVuXNn51heXp6+/vprLVy4UDk5OfLw8LjhNgrS7qFDh8q73DIXEBCgVq1aFVt7SEiIMjIyXMYyMjIUEhJSEeWVqZv1er2q9L6GhoYqIiLCZaxt27b67LPPil2nuPfWbrfLz8+vXOosC6XptSh33323vvnmm7IsrVz9+OOP2rBhg1auXHnDedVhn73VXq9XlfbZF154wXnURJLat2+vH3/8UXPmzFFsbGyR61TVfVYqXb9FKel+y1dmJdCnTx+lpaUpNTXV+ejatauGDRum1NTUm4Yh6b8BKi0tTaGhoRVQcdm6cOGCDh8+XGztkZGRSkpKchlbv369IiMjK6K8MnWzXq9Xld7XX//61y5X2kjSDz/8oKZNmxa7TlV9b0vTa1FSU1OrxHtbYNmyZQoKClJMTMwN51XV9/Vat9rr9arSPnvx4kXVqOH6z7WHh4fy8/OLXacqv7el6bcoJd5vS3ooC66u/8ps+PDh1osvvuh8PmvWLCsxMdE6fPiwlZKSYg0dOtTy9fW19u7d64ZqS+b555+3Nm/ebB09etT69ttvraioKKtBgwZWZmamZVmFe/32228tT09P6/XXX7f27dtnzZgxw/Ly8rLS0tLc1cItK2mvVfl93b59u+Xp6Wm98sor1sGDB60PP/zQqlmzpvXBBx8457z44ovW8OHDnc+PHDli1axZ03rhhResffv2WYsWLbI8PDysdevWuaOFW1aaXufPn2+tWrXKOnjwoJWWlmaNHz/eqlGjhrVhwwZ3tFBieXl5VpMmTawpU6YUWlad9lnLKlmvVXmfjY2NtRo1amStWbPGOnr0qLVy5UqrQYMG1uTJk51zqss+a1ml67cs9lsC0W26PhD17NnTio2NdT6fMGGC1aRJE8vb29sKDg62HnzwQWvXrl0VX2gpPProo1ZoaKjl7e1tNWrUyHr00UetQ4cOOZdf36tlWdYnn3xitWrVyvL29rbatWtnJSQkVHDVpVPSXqvy+2pZlrV69WrrzjvvtHx8fKw2bdpY77zzjsvy2NhYq2fPni5jmzZtsjp16mR5e3tbzZs3t5YtW1ZxBd+Gkvb617/+1WrRooXl6+tr1atXz7r//vutjRs3VnDVpZeYmGhJsg4cOFBoWXXaZy2rZL1W5X3W4XBY48ePt5o0aWL5+vpazZs3t/70pz9ZOTk5zjnVaZ8tTb9lsd/aLOsmt7oEAACo5jiHCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAKPYbDatWrXqludv3rxZNptNWVlZ5VYTAPcjEAGoFp566inZbDbZbDZ5eXkpODhYffv21dKlS11+B9Lp06fVv3//W95ujx49dPr0afn7+0uS4uPjFRAQUNblA3AzAhGAauOBBx7Q6dOndezYMa1du1a9evXS+PHjNWDAAF29elXSf38LuI+Pzy1v09vbWyEhIbLZbOVVNoBKgEAEoNrw8fFRSEiIGjVqpM6dO+ull17SF198obVr1yo+Pl5S4a/Mtm7dqk6dOsnX11ddu3bVqlWrZLPZlJqaKsn1K7PNmzdrxIgRys7Odh6NmjlzZoX3CaDsEYgAVGu9e/dWx44dtXLlykLLHA6HHnroIbVv3167du3Syy+/rClTphS7rR49eujNN9+U3W7X6dOndfr0af3xj38sz/IBVBBPdxcAAOWtTZs2+v777wuNr1ixQjabTe+++658fX0VERGhkydP6umnny5yO97e3vL395fNZlNISEh5lw2gAnGECEC1Z1lWkecAHThwQB06dJCvr69z7O67767I0gBUEgQiANXevn371KxZM3eXAaASIxABqNY2btyotLQ0DR48uNCy1q1bKy0tTTk5Oc6xHTt23HB73t7eysvLK/M6AbgXgQhAtZGTk6P09HSdPHlSu3bt0l/+8hc9/PDDGjBggJ588slC8x9//HHl5+fr97//vfbt26fExES9/vrrklTsZfbh4eG6cOGCkpKS9PPPP+vixYvl2hOAikEgAlBtrFu3TqGhoQoPD9cDDzygTZs26a233tIXX3whDw+PQvPtdrtWr16t1NRUderUSX/60580ffp0SXI5r+haPXr00LPPPqtHH31UgYGBmjt3brn2BKBi2CzLstxdBABUFh9++KHzXkN+fn7uLgdABeGyewBGe//999W8eXM1atRIu3fv1pQpU/S73/2OMAQYhkAEwGjp6emaPn260tPTFRoaqt/+9rd65ZVX3F0WgArGV2YAAMB4nFQNAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIz3/wG1AUfqHWKd6gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_digits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of samples: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(train_set))\n\u001b[1;32m     34\u001b[0m digit_distribution(train_set)\n\u001b[0;32m---> 35\u001b[0m plot_digits(train_set, preprocess)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_digits' is not defined"
     ]
    }
   ],
   "source": [
    "# create and select training dataset\n",
    "# original 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='train',\n",
    "#                             color=True,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             first_color_max_nr=4,\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='train',\n",
    "#                             color=False,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "train_set = DatasetMNIST(root='./data',\n",
    "                            env='train',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            preprocess=preprocess,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE)\n",
    "\n",
    "print(\"Number of samples: \", len(train_set))\n",
    "digit_distribution(train_set)\n",
    "plot_digits(train_set, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select validation dataset\n",
    "# original 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='val',\n",
    "#                             color=True,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             first_color_max_nr=4,\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='val',\n",
    "#                             color=False,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "val_set = DatasetMNIST(root='./data',\n",
    "                            env='val',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            preprocess=preprocess,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE)\n",
    "\n",
    "print(\"Number of samples: \", len(val_set))\n",
    "digit_distribution(val_set)\n",
    "plot_digits(val_set, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select test dataset\n",
    "# original 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='test',\n",
    "#                             color=True,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             first_color_max_nr=4,\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='test',\n",
    "#                             color=False,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "test_set = DatasetMNIST(root='./data',\n",
    "                            env='test',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            preprocess=preprocess,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE)\n",
    "\n",
    "print(\"Number of samples: \", len(test_set))\n",
    "digit_distribution(test_set)\n",
    "plot_digits(test_set, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select test_fool dataset\n",
    "# original 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='test_fool',\n",
    "#                             color=True,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             first_color_max_nr=4,\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "# DatasetMNIST(root='./data',\n",
    "#                             env='test_fool',\n",
    "#                             color=False,\n",
    "#                             opt_postfix=\"10classes\",\n",
    "#                             preprocess=preprocess,\n",
    "#                             transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "test_set_fool = DatasetMNIST(root='./data',\n",
    "                            env='test_fool',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            preprocess=preprocess,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_fool_loader = torch.utils.data.DataLoader(dataset=test_set_fool,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE)\n",
    "\n",
    "print(\"Number of samples: \", len(test_set_fool))\n",
    "digit_distribution(test_set_fool)\n",
    "plot_digits(test_set_fool, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic checks for correct labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if all test labels are correct (all high_low labels are equal to the color labels -> No messages expected)\n",
    "for idx, i in enumerate(test_loader):\n",
    "    if not ((i[2] == i[3]).all().item()):\n",
    "        print(\"Error! in batch \", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if all test_fool labels are correct (all high_low labels are opposite to the color labels -> No messages expected)\n",
    "for idx, i in enumerate(test_fool_loader):\n",
    "    if not ((i[2] != i[3]).all().item()):\n",
    "        print(\"Error! in batch \", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# some checks with explanations\n",
    "if not DATASET_SHUFFLE:\n",
    "    # only make sense, if datasets are not shuffled\n",
    "    for i, (data1, data2) in enumerate(zip(itertools.cycle(test_loader), test_fool_loader)):\n",
    "        print(f\"Same image data: {(data1[0] == data2[0]).all().item()}, because the same digit is either on red or green color channel!\")\n",
    "        print(f\"Same ground truth label: {(data1[1] == data2[1]).all().item()}, because it's the same digit, independent from its representation\")\n",
    "        print(f\"Same low-high label: {(data1[2] == data2[2]).all().item()}, because it's the same digit, independent from its representation\")\n",
    "        print(f\"Same color label: {(data1[3] == data2[3]).all().item()}, because the test and test-fool sets have opposite colors\")\n",
    "        break\n",
    "\n",
    "    # index within batch\n",
    "    idx = 42\n",
    "    \n",
    "    # show same image of same digit from both datasets\n",
    "    normalizer = preprocess.transforms.copy().pop()\n",
    "    img1 = torch.stack((data1[0][idx][0]*torch.Tensor(normalizer.std)[0] + torch.Tensor(normalizer.mean)[0],\n",
    "                        data1[0][idx][1]*torch.Tensor(normalizer.std)[1] + torch.Tensor(normalizer.mean)[1],\n",
    "                        data1[0][idx][2]*torch.Tensor(normalizer.std)[2] + torch.Tensor(normalizer.mean)[2]))\n",
    "\n",
    "    img2 = torch.stack((data2[0][idx][0]*torch.Tensor(normalizer.std)[0] + torch.Tensor(normalizer.mean)[0],\n",
    "                        data2[0][idx][1]*torch.Tensor(normalizer.std)[1] + torch.Tensor(normalizer.mean)[1],\n",
    "                        data2[0][idx][2]*torch.Tensor(normalizer.std)[2] + torch.Tensor(normalizer.mean)[2]))\n",
    "\n",
    "    plt.imshow(np.transpose(img1.cpu().numpy(), (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(np.transpose(img2.cpu().numpy(), (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "    del normalizer\n",
    "    del img1\n",
    "    del img2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device (For number crunching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 transfer learning\n",
    "# https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/2\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# replace the last fully-connected layer\n",
    "# parameters of newly constructed modules have required_grad=True by default\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "model.to(device)\n",
    "model.fc.weight # initialized with random numbers - Requires grad is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "weight_backup = copy.deepcopy(nn.Linear(2048, 2))\n",
    "weight_backup.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat from here\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = copy.deepcopy(weight_backup)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=3):\n",
    "    \"\"\"\n",
    "    Training procedure and performance measurement of the model.\n",
    "    \"\"\"\n",
    "    history = {'train_w_backprop'   : {'loss' : [],\n",
    "                                       'acc' : [],\n",
    "                                       'logits' : [],\n",
    "                                       'labels' : []},\n",
    "               'train'              : {'loss' : [],\n",
    "                                       'acc' : [],\n",
    "                                       'logits' : [],\n",
    "                                       'labels' : []},\n",
    "                'validation'        : {'loss' : [],\n",
    "                                       'acc' : [],\n",
    "                                       'logits' : [],\n",
    "                                       'labels' : []},\n",
    "                'test'              : {'loss' : [],\n",
    "                                       'acc' : [],\n",
    "                                       'logits' : [],\n",
    "                                       'labels' : []},\n",
    "                'test_fool'         : {'loss' : [],\n",
    "                                       'acc' : [],\n",
    "                                       'logits' : [],\n",
    "                                       'labels' : []}}\n",
    "    \n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    dummy_30gb = True\n",
    "    dummy_20gb = True\n",
    "    dummy_10gb = True\n",
    "        \n",
    "    # epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # phase\n",
    "        for phase in history.keys():\n",
    "            if phase == 'train_w_backprop':\n",
    "                print(f\"Start phase {phase}\")\n",
    "                model.train()\n",
    "                datasource = train_loader\n",
    "            elif phase == \"train\":\n",
    "                print(f\"Start phase {phase}\")\n",
    "                model.eval()\n",
    "                datasource = train_loader\n",
    "            elif phase == \"validation\":\n",
    "                print(f\"Start phase {phase}\")\n",
    "                model.eval()\n",
    "                datasource = val_loader\n",
    "            elif phase == \"test\":\n",
    "                print(f\"Start phase {phase}\")\n",
    "                model.eval()\n",
    "                datasource = test_loader\n",
    "            elif phase == \"test_fool\":\n",
    "                print(f\"Start phase {phase}\")\n",
    "                model.eval()\n",
    "                datasource = test_fool_loader\n",
    "            else:\n",
    "                raise NotImplementedError(\"Sorry, unknown phase!\")\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            label_counter = 0\n",
    "\n",
    "            # mini-batches\n",
    "            for inputs, _, low_high_lables, _ in datasource:\n",
    "                \n",
    "                # inference\n",
    "                inputs = inputs.to(device)\n",
    "                low_high_lables = low_high_lables.to(device)\n",
    "                if phase == 'train_w_backprop':\n",
    "                    logits = model(inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        logits = model(inputs)\n",
    "                history[phase]['logits'].extend(logits.cpu())\n",
    "                history[phase]['labels'].extend(low_high_lables.cpu())\n",
    "                \n",
    "                # sanity check: ensure only one unique max for next step\n",
    "                unique_max = (logits == torch.max(logits)).nonzero().shape == torch.Size([1, 2])\n",
    "                if (not unique_max):\n",
    "                    print(\"Error: not unique max! Take first!\")\n",
    "                \n",
    "                # prediction\n",
    "                preds = logits.argmax(dim=1)\n",
    "                running_corrects += torch.sum(preds == low_high_lables.data)\n",
    "                label_counter += low_high_lables.size()[0]\n",
    "                \n",
    "                # loss\n",
    "                batch_loss = criterion(logits, low_high_lables)\n",
    "                epoch_loss += batch_loss.item()\n",
    "\n",
    "                # training\n",
    "                if phase == 'train_w_backprop':\n",
    "                    optimizer.zero_grad()   # Sets the gradients of all optimized torch.Tensor to zero.\n",
    "                    batch_loss.backward()   # compute gradients\n",
    "                    optimizer.step()        # Performs a single optimization step (parameter update).\n",
    "                \n",
    "                # analyze free gpu memory\n",
    "                info = nvmlDeviceGetMemoryInfo(h)\n",
    "                if (dummy_30gb and info.free < 30000000000):\n",
    "                    print(\"More than 30GB in use!\")\n",
    "                    dummy_30gb = False\n",
    "                elif (dummy_20gb and info.free < 20000000000):\n",
    "                    print(\"More than 20GB in use!\")\n",
    "                    dummy_20gb = False\n",
    "                elif (dummy_10gb and info.free < 10000000000):\n",
    "                    print(\"More than 10GB in use!\")\n",
    "                    dummy_10gb = False\n",
    "                        \n",
    "            epoch_acc = 100 * running_corrects.double() / label_counter\n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "            \n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc.cpu())\n",
    "            \n",
    "            # del inputs\n",
    "            # del low_high_lables\n",
    "            torch.cuda.empty_cache()                                   \n",
    "            \n",
    "    return model, history, inputs, low_high_lables, logits, batch_loss, label_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.sigmoid (Map values between 0 and 1) + F.binary_cross_entropy\n",
    "# https://zhang-yang.medium.com/how-is-pytorchs-binary-cross-entropy-with-logits-function-related-to-sigmoid-and-d3bd8fb080e7\n",
    "# https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "#criterion = F.binary_cross_entropy_with_logits #(input, target)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one epoch takes around ~1 min and 8s\n",
    "model, history, inputs, labels, preds, batch_loss, label_counter = train_model(model, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#pickle.dump(model, open(\"/home/patrick.koller/masterthesis/data/models/standalone_resnet50.mdl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist, show_curves=['train_w_backprop', 'train', 'validation', 'test', 'test_fool']):\n",
    "    \"\"\"\n",
    "    Plot the losses and accuracies during the training, validation and test procedures.\n",
    "    \"\"\"\n",
    "    plt.subplot(1,2,1)\n",
    "    if 'train_w_backprop' in show_curves:\n",
    "        plt.semilogy(range(len(hist['train_w_backprop']['loss'])), hist['train_w_backprop']['loss'], label='Train batch accumulated')\n",
    "    if 'train' in show_curves:\n",
    "        plt.semilogy(range(len(hist['train']['loss'])), hist['train']['loss'], label='Train')\n",
    "    if 'validation' in show_curves:\n",
    "        plt.semilogy(range(len(hist['validation']['loss'])), hist['validation']['loss'], label='Validation')\n",
    "    if 'test' in show_curves:\n",
    "        plt.semilogy(range(len(hist['test']['loss'])), hist['test']['loss'], label='Test')\n",
    "    if 'test_fool' in show_curves:\n",
    "        plt.semilogy(range(len(hist['test_fool']['loss'])), hist['test_fool']['loss'], label='Test fool')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    if 'train_w_backprop' in show_curves:\n",
    "        plt.plot(range(len(hist['train_w_backprop']['acc'])), hist['train_w_backprop']['acc'], label='Train batch accumulated')\n",
    "    if 'train' in show_curves:\n",
    "        plt.plot(range(len(hist['train']['acc'])), hist['train']['acc'], label='Train')\n",
    "    if 'validation' in show_curves:\n",
    "        plt.plot(range(len(hist['validation']['acc'])), hist['validation']['acc'], label='Validation')\n",
    "    if 'test' in show_curves:\n",
    "        plt.plot(range(len(hist['test']['acc'])), hist['test']['acc'], label='Test')\n",
    "    if 'test_fool' in show_curves:\n",
    "        plt.plot(range(len(hist['test_fool']['acc'])), hist['test_fool']['acc'], label='Test fool')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy in %')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history, show_curves=['train_w_backprop', 'train', 'validation', 'test', 'test_fool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the validation accuracy higher than the training accuracy?  \n",
    "\n",
    "Answer, if a dropout layer is in use: (There is no dropout layer in resnet50)  \n",
    "This is a typical behaviour when using dropout, since the behaviour during training and testing are different. When training, a percentage of the features are set to zero (50%, if dropout(0.5)). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies.\n",
    "\n",
    "Answer for this resnet50:  \n",
    "Most probably, the training (50000 samples) and the validation (5000 samples) datasets are not 100% i.i.d. from the same distribution (or at least they are unbalanced!) and there are some less complicated samples in the smaller dataset. This usually is the case, if the validation loss is smaller than the training loss. This hypothesis needs to be proven! This can be confirmed in a simple and easy way by switching the training and the validation (5000 samples) dataset and validate on the training (50000 samples) dataset. In this case the training loss curve (obtained with the validation dataset) is lower than the validation loss curve (obtained with the training dataset)! Of course, this is not a 100% perfect proof, but a simple and easy indicator to support this hypothesis. If this should be a problem, more analysis of the data needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(train_loader, model, device, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(val_loader, model, device, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(test_loader, model, device, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(test_fool_loader, model, device, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4970e4b695934d4c8057fb9814a5fc9d1acefab431f21ea67995a21a78a4c555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
