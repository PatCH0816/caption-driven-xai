{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fooled colored MNIST (Try to put more focus on colors instead of the shape of the digits)\n",
    "The idea is to create a model, which should be able to asses, if the digit in the image is a low or a high number. The image dataset of colored digits is divided into three parts namely the train, validation and test datasets. In the train and validatin datasets, the low numbers are colored in red and the high numbers are colored in green. In the test dataset, the colors are random. If the model is able to recognize the value of the digits from it's shape, the performance should be nearly equal as the performance on the train and validation datasets. The hypothesis is, that the model will learn to separate low from high digits based on their color and therefore will fail on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# pytorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision import __version__ as torchvision_version\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include plots in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check environment\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"Pytorch version: \", torch.__version__)\n",
    "print(\"Torchvision version: \", torchvision_version)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from utils.mnist_plotting import *\n",
    "\n",
    "# dataset parameters\n",
    "DATASET_BATCH_SIZE = 128\n",
    "DATASET_SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select training dataset\n",
    "# original 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='train',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            first_color_max_nr=4,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='train',\n",
    "                            color=False,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "train_set = DatasetMNIST(root='./data',\n",
    "                            env='train',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "print(\"Number of samples: \", len(train_set))\n",
    "plot_digits(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select validation dataset\n",
    "# original 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='val',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            first_color_max_nr=4,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='val',\n",
    "                            color=False,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "val_set = DatasetMNIST(root='./data',\n",
    "                            env='val',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "print(\"Number of samples: \", len(val_set))\n",
    "plot_digits(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select test dataset\n",
    "# original 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='test',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            first_color_max_nr=4,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='test',\n",
    "                            color=False,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "test_set = DatasetMNIST(root='./data',\n",
    "                            env='test',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "print(\"Number of samples: \", len(test_set))\n",
    "plot_digits(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and select test_fool dataset\n",
    "# original 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='test_fool',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            first_color_max_nr=4,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# gray 10 class problem\n",
    "DatasetMNIST(root='./data',\n",
    "                            env='test_fool',\n",
    "                            color=False,\n",
    "                            opt_postfix=\"10classes\",\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# simplified binary problem\n",
    "test_set_fool = DatasetMNIST(root='./data',\n",
    "                            env='test_fool',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_fool_loader = torch.utils.data.DataLoader(dataset=test_set_fool,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "print(\"Number of samples: \", len(test_set_fool))\n",
    "plot_digits(test_set_fool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if all test labels are correct (all high_low labels are equal to the color labels -> No messages expected)\n",
    "for idx, i in enumerate(test_loader):\n",
    "    if not ((i[2] == i[3]).all().item()):\n",
    "        print(\"Error! in batch \", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if all test_fool labels are correct (all high_low labels are opposite to the color labels -> No messages expected)\n",
    "for idx, i in enumerate(test_fool_loader):\n",
    "    if not ((i[2] != i[3]).all().item()):\n",
    "        print(\"Error! in batch \", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device (For number crunching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet50 transfer learning\n",
    "# https://discuss.pytorch.org/t/how-to-modify-the-final-fc-layer-based-on-the-torch-model/766/2\n",
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# replace the last fully-connected layer\n",
    "# parameters of newly constructed modules have required_grad=True by default\n",
    "model.fc = nn.Linear(2048, 2)\n",
    "model.to(device)\n",
    "model.fc.weight # initialized with random numbers - Requires grad is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=3):\n",
    "    \"\"\"\n",
    "    Training procedure and performance measurement of the model.\n",
    "    \"\"\"\n",
    "    history = {'train_w_backprop'   : {'loss' : [],\n",
    "                                       'acc' : []},\n",
    "               'train'              : {'loss' : [],\n",
    "                                       'acc' : []},\n",
    "                'validation'        : {'loss' : [],\n",
    "                                       'acc' : []},\n",
    "                'test'              : {'loss' : [],\n",
    "                                       'acc' : []},\n",
    "                'test_fool'         : {'loss' : [],\n",
    "                                       'acc' : []}}\n",
    "        \n",
    "    # epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # phase\n",
    "        for phase in history.keys():\n",
    "            if phase == 'train_w_backprop':\n",
    "                model.train()\n",
    "                datasource = train_loader\n",
    "            elif phase == \"train\":\n",
    "                model.eval()\n",
    "                datasource = train_loader\n",
    "            elif phase == \"validation\":\n",
    "                model.eval()\n",
    "                datasource = val_loader\n",
    "            elif phase == \"test\":\n",
    "                model.eval()\n",
    "                datasource = test_loader\n",
    "            elif phase == \"test_fool\":\n",
    "                model.eval()\n",
    "                datasource = test_fool_loader\n",
    "            else:\n",
    "                raise NotImplementedError(\"Sorry, unknown phase!\")\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            label_counter = 0\n",
    "\n",
    "            # mini-batches\n",
    "            for inputs, _, low_high_lables, _ in datasource:\n",
    "                inputs = inputs.to(device)\n",
    "                low_high_lables = low_high_lables.to(device)\n",
    "                \n",
    "                logits = model(inputs)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                running_corrects += torch.sum(preds == low_high_lables.data)\n",
    "                label_counter += low_high_lables.size()[0]\n",
    "                \n",
    "                batch_loss = criterion(logits, low_high_lables)\n",
    "                epoch_loss += batch_loss.item()\n",
    "\n",
    "                if phase == 'train_w_backprop':\n",
    "                    optimizer.zero_grad()   # Sets the gradients of all optimized torch.Tensor to zero.\n",
    "                    batch_loss.backward()   # compute gradients\n",
    "                    optimizer.step()        # Performs a single optimization step (parameter update).\n",
    "                \n",
    "            epoch_acc = 100 * running_corrects.double() / label_counter\n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "            \n",
    "            history[phase]['loss'].append(epoch_loss)\n",
    "            history[phase]['acc'].append(epoch_acc.cpu())\n",
    "            \n",
    "    return model, history, inputs, low_high_lables, logits, batch_loss, label_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.sigmoid (Map values between 0 and 1) + F.binary_cross_entropy\n",
    "# https://zhang-yang.medium.com/how-is-pytorchs-binary-cross-entropy-with-logits-function-related-to-sigmoid-and-d3bd8fb080e7\n",
    "# https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "#criterion = F.binary_cross_entropy_with_logits #(input, target)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history, inputs, labels, preds, batch_loss, label_counter = train_model(model, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist, show_curves=['train_w_backprop', 'train', 'validation', 'test', 'test_fool']):\n",
    "    \"\"\"\n",
    "    Plot the losses and accuracies during the training, validation and test procedures.\n",
    "    \"\"\"\n",
    "    plt.subplot(1,2,1)\n",
    "    if 'train_w_backprop' in show_curves:\n",
    "        plt.semilogy(range(len(hist['train_w_backprop']['loss'])), hist['train_w_backprop']['loss'], label='Train batch accumulated')\n",
    "    if 'train' in show_curves:\n",
    "        plt.semilogy(range(len(hist['train']['loss'])), hist['train']['loss'], label='Train')\n",
    "    if 'validation' in show_curves:\n",
    "        plt.semilogy(range(len(hist['validation']['loss'])), hist['validation']['loss'], label='Validation')\n",
    "    if 'test' in show_curves:\n",
    "        plt.semilogy(range(len(hist['test']['loss'])), hist['test']['loss'], label='Test')\n",
    "    if 'test_fool' in show_curves:\n",
    "        plt.semilogy(range(len(hist['test_fool']['loss'])), hist['test_fool']['loss'], label='Test fool')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    if 'train_w_backprop' in show_curves:\n",
    "        plt.plot(range(len(hist['train_w_backprop']['acc'])), hist['train_w_backprop']['acc'], label='Train batch accumulated')\n",
    "    if 'train' in show_curves:\n",
    "        plt.plot(range(len(hist['train']['acc'])), hist['train']['acc'], label='Train')\n",
    "    if 'validation' in show_curves:\n",
    "        plt.plot(range(len(hist['validation']['acc'])), hist['validation']['acc'], label='Validation')\n",
    "    if 'test' in show_curves:\n",
    "        plt.plot(range(len(hist['test']['acc'])), hist['test']['acc'], label='Test')\n",
    "    if 'test_fool' in show_curves:\n",
    "        plt.plot(range(len(hist['test_fool']['acc'])), hist['test_fool']['acc'], label='Test fool')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy in %')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history, show_curves=['train_w_backprop', 'train', 'validation', 'test', 'test_fool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the validation accuracy higher than the training accuracy?  \n",
    "\n",
    "Answer, if a dropout layer is in use: (There is no dropout layer in resnet50)  \n",
    "This is a typical behaviour when using dropout, since the behaviour during training and testing are different. When training, a percentage of the features are set to zero (50%, if dropout(0.5)). When testing, all features are used (and are scaled appropriately). So the model at test time is more robust - and can lead to higher testing accuracies.\n",
    "\n",
    "Answer for this resnet50:  \n",
    "Most probably, the training (50000 samples) and the validation (5000 samples) datasets are not 100% i.i.d. from the same distribution (or at least they are unbalanced!) and there are some less complicated samples in the smaller dataset. This usually is the case, if the validation loss is smaller than the training loss. This hypothesis needs to be proven! This can be confirmed in a simple and easy way by switching the training and the validation (5000 samples) dataset and validate on the training (50000 samples) dataset. In this case the training loss curve (obtained with the validation dataset) is lower than the validation loss curve (obtained with the training dataset)! Of course, this is not a 100% perfect proof, but a simple and easy indicator to support this hypothesis. If this should be a problem, more analysis of the data needs to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(train_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(val_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tests(test_fool_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4970e4b695934d4c8057fb9814a5fc9d1acefab431f21ea67995a21a78a4c555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
