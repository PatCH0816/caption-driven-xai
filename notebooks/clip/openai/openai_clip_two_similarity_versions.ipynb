{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: sx-el-121920\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "print(\"Hostname: \" + socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.0a0+d0d6b1f\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from utils.mnist_plotting import *\n",
    "\n",
    "# dataset parameters\n",
    "DATASET_BATCH_SIZE = 128\n",
    "DATASET_SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# initialize datasets\n",
    "train_set = DatasetMNIST(root='./data',\n",
    "                            env='train',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_set = DatasetMNIST(root='./data',\n",
    "                            env='val',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set = DatasetMNIST(root='./data',\n",
    "                            env='test',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_fool = DatasetMNIST(root='./data',\n",
    "                            env='test_fool',\n",
    "                            color=True,\n",
    "                            opt_postfix=\"2classes\",\n",
    "                            filter=[5,8],\n",
    "                            first_color_max_nr=5,\n",
    "                            transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n",
    "\n",
    "test_fool_loader = torch.utils.data.DataLoader(dataset=test_set_fool,\n",
    "                                            batch_size=DATASET_BATCH_SIZE,\n",
    "                                            shuffle=DATASET_SHUFFLE,\n",
    "                                            num_workers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 9425\n",
      "Number of validation samples: 1888\n",
      "Number of test samples: 1866\n",
      "Number of test fool samples: 1866\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_loader.dataset.data_label_tuples)}\")\n",
    "print(f\"Number of validation samples: {len(val_loader.dataset.data_label_tuples)}\")\n",
    "print(f\"Number of test samples: {len(test_loader.dataset.data_label_tuples)}\")\n",
    "print(f\"Number of test fool samples: {len(test_fool_loader.dataset.data_label_tuples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset.data_label_tuples) % DATASET_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device (For number crunching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.cuda().eval()\n",
    "\n",
    "#mnist_template = 'a photo of the number: \"{}\".'\n",
    "mnist_template = ['a photo of the number: \"{}\".', 'a photo of a red number: \"{}\".', 'a photo a green number: \"{}\".']\n",
    "mnist_classes = [\"5\", \"8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of the number: \"5\".',\n",
       " 'a photo of a red number: \"5\".',\n",
       " 'a photo a green number: \"5\".',\n",
       " 'a photo of the number: \"8\".',\n",
       " 'a photo of a red number: \"8\".',\n",
       " 'a photo a green number: \"8\".']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build text strings\n",
    "text_descriptions = []\n",
    "\n",
    "for digit in mnist_classes:\n",
    "    text_descriptions.extend([template.format(digit) for template in mnist_template])\n",
    "    \n",
    "text_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build text features\n",
    "text_tokens = clip.tokenize(text_descriptions).cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1024])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.shape # shape [number of texts times 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2710,  0.3672,  0.3064,  ..., -0.1648,  0.0879,  0.1384],\n",
       "        [ 0.3301,  0.2534,  0.1445,  ..., -0.0715,  0.0442,  0.1587],\n",
       "        [ 0.2252,  0.2289,  0.3535,  ..., -0.0059, -0.1753,  0.1807],\n",
       "        [ 0.0952,  0.2510, -0.0042,  ..., -0.2423,  0.0435,  0.0859],\n",
       "        [ 0.1028,  0.1400, -0.1921,  ..., -0.1677,  0.0241,  0.1140],\n",
       "        [ 0.0689,  0.1344,  0.0947,  ..., -0.1091, -0.1842,  0.1322]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, ground_truth_label, low_high_label, color_label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # preprocess images\n",
    "    images_new = []\n",
    "    for img in images:\n",
    "        images_new.append(preprocess(transform(img)))\n",
    "\n",
    "    # building image features\n",
    "    images = torch.tensor(np.stack(images_new)).cuda()\n",
    "    \n",
    "    # predict\n",
    "    image_features = model.encode_image(images)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarities = image_features @ text_features.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1024])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape # 128 images in batch times 1024 feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 6])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also experimented with ensembling over multiple zeroshot classifiers as another way of improving performance.\n",
    "These classifiers are computed by using different context\n",
    "prompts such as ‘A photo of a big {label}” and\n",
    "“A photo of a small {label}”. We construct the\n",
    "ensemble over the embedding space instead of probability\n",
    "space. This allows us to cache a single set of averaged text\n",
    "embeddings so that the compute cost of the ensemble is the\n",
    "same as using a single classifier when amortized over many\n",
    "predictions. We’ve observed ensembling across many generated zero-shot classifiers to reliably improve performance\n",
    "and use it for the majority of datasets. On ImageNet, we\n",
    "ensemble 80 different context prompts and this improves\n",
    "performance by an additional 3.5% over the single default\n",
    "prompt discussed above. When considered together, prompt\n",
    "engineering and ensembling improve ImageNet accuracy\n",
    "by almost 5%. In Figure 4 we visualize how prompt engineering and ensembling change the performance of a set of\n",
    "CLIP models compared to the contextless baseline approach\n",
    "of directly embedding the class name as done in Li et al.\n",
    "(2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_feature_generator1234(clip_version, model, classnames, class_template):\n",
    "    \"\"\"\n",
    "    Generates the text-feature matrix from given template sentences and classes and place it on the GPU.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_features = []\n",
    "        for classname in classnames:\n",
    "            texts = [template.format(classname) for template in class_template] # generate texts using templates with classes\n",
    "            texts = clip_version.tokenize(texts).cuda() # generate text-tokens\n",
    "            class_embeddings = model.encode_text(texts) # generate text embeddings -> torch.Size([nr_templates x 1024])\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True) # normalize feature vector -> torch.Size([nr_templates x 1024])\n",
    "            class_embedding = class_embeddings.mean(dim=0) # average over all template sentences\n",
    "            class_embedding /= class_embedding.norm() # normalize feature vector -> torch.Size([1024])\n",
    "            text_features.append(class_embedding) # generate feature matrix -> torch.Size([nr_classes x 1024])\n",
    "        text_features = torch.stack(text_features, dim=1).cuda()\n",
    "    return text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature_generator1234(clip, model, mnist_classes, mnist_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
