{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.0a0+d0d6b1f\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n",
      "MNIST dataset already exists\n"
     ]
    }
   ],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from torchvision import transforms\n",
    "\n",
    "# parameters\n",
    "size_of_batch = 128\n",
    "\n",
    "# dataset preparation\n",
    "train_set_gray = DatasetMNIST(root='./data',\n",
    "                       env='train',\n",
    "                       color=False,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_set_gray = DatasetMNIST(root='./data',\n",
    "                       env='val',\n",
    "                       color=False,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_gray = DatasetMNIST(root='./data',\n",
    "                       env='test',\n",
    "                       color=False,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_set_color = DatasetMNIST(root='./data',\n",
    "                       env='train',\n",
    "                       color=True,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_set_color = DatasetMNIST(root='./data',\n",
    "                       env='val',\n",
    "                       color=True,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set_color = DatasetMNIST(root='./data',\n",
    "                       env='test',\n",
    "                       color=True,\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# dataloaders\n",
    "train_loader_gray = torch.utils.data.DataLoader(dataset=train_set_gray,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "val_loader_gray = torch.utils.data.DataLoader(dataset=val_set_gray,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "test_loader_gray = torch.utils.data.DataLoader(dataset=test_set_gray,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "train_loader_color = torch.utils.data.DataLoader(dataset=train_set_color,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "val_loader_color = torch.utils.data.DataLoader(dataset=val_set_color,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "test_loader_color = torch.utils.data.DataLoader(dataset=test_set_color,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN50-quickgelu',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'RN101',\n",
       " 'RN101-quickgelu',\n",
       " 'roberta-ViT-B-32',\n",
       " 'timm-convnext_base',\n",
       " 'timm-convnext_large',\n",
       " 'timm-convnext_xlarge',\n",
       " 'timm-efficientnetv2_rw_s',\n",
       " 'timm-resnetaa50d',\n",
       " 'timm-swin_base_patch4_window7_224',\n",
       " 'timm-vit_medium_patch16_gap_256',\n",
       " 'timm-vit_relpos_medium_patch16_cls_224',\n",
       " 'ViT-B-16',\n",
       " 'ViT-B-16-plus',\n",
       " 'ViT-B-16-plus-240',\n",
       " 'ViT-B-32',\n",
       " 'ViT-B-32-plus-256',\n",
       " 'ViT-B-32-quickgelu',\n",
       " 'ViT-e-14',\n",
       " 'ViT-G-14',\n",
       " 'ViT-g-14',\n",
       " 'ViT-H-14',\n",
       " 'ViT-H-16',\n",
       " 'ViT-L-14',\n",
       " 'ViT-L-14-280',\n",
       " 'ViT-L-14-336',\n",
       " 'ViT-L-16',\n",
       " 'ViT-L-16-320',\n",
       " 'ViT-M-16',\n",
       " 'ViT-M-16-alt',\n",
       " 'ViT-M-32',\n",
       " 'ViT-M-32-alt',\n",
       " 'ViT-S-16',\n",
       " 'ViT-S-16-alt',\n",
       " 'ViT-S-32',\n",
       " 'ViT-S-32-alt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('RN50x64', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion2b_e16'),\n",
       " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14', 'laion400m_e31'),\n",
       " ('ViT-L-14', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
       " ('ViT-L-14-336', 'openai'),\n",
       " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
       " ('ViT-g-14', 'laion2b_s12b_b42k')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "# [('RN50', 'openai'),\n",
    "#  ('RN50', 'yfcc15m'),\n",
    "#  ('RN50', 'cc12m'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('RN50', pretrained='cc12m')\n",
    "\n",
    "# performances (https://github.com/mlfoundations/open_clip)\n",
    "# Below are checkpoints of models trained on YFCC-15M, along with their zero-shot top-1 accuracies\n",
    "# on ImageNet and ImageNetV2. These models were trained using 8 GPUs and the same hyperparameters\n",
    "# described in the \"Sample running code\" section, with the exception of lr=5e-4 and epochs=32.\n",
    "# For YFCC-15M: -> https://github.com/mlfoundations/open_clip\n",
    "# ResNet-50 (32.7% / 27.9%)\n",
    "# ResNet-101 (34.8% / 30.0%)\n",
    "# For CC12M: -> https://github.com/google-research-datasets/conceptual-12m\n",
    "# ResNet-50 (36.45%)\n",
    "\n",
    "# there is a 400m image/text dataset, but open_clip uses the visiontransformers on these:\n",
    "# https://laion.ai/blog/laion-400-open-dataset/\n",
    "\n",
    "# performance of original clip model:\n",
    "# This is a key change: by not directly optimizing for the benchmark, we show that it becomes much more\n",
    "# representative: our system closes this “robustness gap” by up to 75% while matching the performance of\n",
    "# the original ResNet-507 on ImageNet zero-shot without using any of the original 1.28M labeled examples.\n",
    "# https://openai.com/blog/clip/\n",
    "# imagenet resnet101: 76.2%\n",
    "# clip VIT-L: 76.2%\n",
    "# imagenet V2 resnet101: 64.3%\n",
    "# clip VIT-L V2: 70.1%\n",
    "\n",
    "# We use the 12 datasets from the well-studied evaluation\n",
    "# suite introduced by (Kornblith et al., 2019) and add 15\n",
    "# additional datasets in order to assess the performance of\n",
    "# models on a wider variety of distributions and tasks. These\n",
    "# datasets include MNIST, the Facial Expression Recognition\n",
    "# 2013 dataset (Goodfellow et al., 2015), STL-10 (Coates\n",
    "# et al., 2011), EuroSAT (Helber et al., 2019), the NWPURESISC45 dataset (Cheng et al., 2017), the German Traffic Sign Recognition Benchmark (GTSRB) dataset (Stallkamp et al., 2011), the KITTI dataset (Geiger et al., 2012),\n",
    "# PatchCamelyon (Veeling et al., 2018), the UCF101 action\n",
    "# recognition dataset (Soomro et al., 2012), Kinetics 700 (Carreira et al., 2019), 2,500 random samples of the CLEVR\n",
    "# dataset (Johnson et al., 2017), the Hateful Memes dataset\n",
    "# (Kiela et al., 2020), and the ImageNet-1k dataset (Deng\n",
    "# et al., 2012). \n",
    "\n",
    "# clip paper (page 40)\n",
    "# clip-RN50 on mnist: 98.3%\n",
    "# clip-RN50 on imagenet: 73.3%\n",
    "# RN50 on mnist: 98.3%\n",
    "# RN50 on imagenet: 74.3%\n",
    "\n",
    "# clip paper zero-shot performance (page 43)\n",
    "# clip-ResNet RN50 on mnist: 66.6%\n",
    "# clip-ResNet RN50 on imagenet: 59.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance of the clip model on the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images in skimage to use and their textual descriptions\n",
    "\n",
    "# Results in:\n",
    "# Training accuracy:  33.85%\n",
    "# Validation accuracy:  36.16%\n",
    "# Test accuracy:  34.16%\n",
    "descriptions1 = {\n",
    "    \"0\": \"a number with the value zero\",\n",
    "    \"1\": \"a number with the value one\",\n",
    "    \"2\": \"a number with the value two\",\n",
    "    \"3\": \"a number with the value three\",\n",
    "    \"4\": \"a number with the value four\",\n",
    "    \"5\": \"a number with the value five\",\n",
    "    \"6\": \"a number with the value six\",\n",
    "    \"7\": \"a number with the value seven\",\n",
    "    \"8\": \"a number with the value eight\",\n",
    "    \"9\": \"a number with the value nine\"\n",
    "}\n",
    "\n",
    "# Results in:\n",
    "# Training accuracy:  33.85%\n",
    "# Validation accuracy:  36.16%\n",
    "# Test accuracy:  34.16%\n",
    "descriptions2 = {\n",
    "    \"0\": 'a photo of the number: \"0\".',\n",
    "    \"1\": 'a photo of the number: \"1\".',\n",
    "    \"2\": 'a photo of the number: \"2\".',\n",
    "    \"3\": 'a photo of the number: \"3\".',\n",
    "    \"4\": 'a photo of the number: \"4\".',\n",
    "    \"5\": 'a photo of the number: \"5\".',\n",
    "    \"6\": 'a photo of the number: \"6\".',\n",
    "    \"7\": 'a photo of the number: \"7\".',\n",
    "    \"8\": 'a photo of the number: \"8\".',\n",
    "    \"9\": 'a photo of the number: \"9\".',\n",
    "}\n",
    "\n",
    "descriptions3 = {\n",
    "    \"0\": '0',\n",
    "    \"1\": '1',\n",
    "    \"2\": '2',\n",
    "    \"3\": '3',\n",
    "    \"4\": '4',\n",
    "    \"5\": '5',\n",
    "    \"6\": '6',\n",
    "    \"7\": '7',\n",
    "    \"8\": '8',\n",
    "    \"9\": '9',\n",
    "}\n",
    "\n",
    "descriptions4 = {\n",
    "    \"0\": 'zero',\n",
    "    \"1\": 'one',\n",
    "    \"2\": 'two',\n",
    "    \"3\": 'three',\n",
    "    \"4\": 'four',\n",
    "    \"5\": 'five',\n",
    "    \"6\": 'six',\n",
    "    \"7\": 'seven',\n",
    "    \"8\": 'eight',\n",
    "    \"9\": 'nine',\n",
    "}\n",
    "\n",
    "descriptions = descriptions4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# len(=nr_of_batches)*batch_size=nr_of_samples\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# len(val_loader)-1 full batches with a size of 128 images\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# one last batch with the remaining <128 images\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#(len(val_loader)-1) * 128 + 16 \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43masses_clip_performance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_gray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining gray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m asses_clip_performance(model, preprocess, val_loader_gray, descriptions, dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation gray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m asses_clip_performance(model, preprocess, test_loader_gray, descriptions, dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest gray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/masterthesis/utils/clip_utils.py:52\u001b[0m, in \u001b[0;36masses_clip_performance\u001b[0;34m(model, preprocess, data_loader, text_descriptions, dataset_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m# inference\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 52\u001b[0m     image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode_image(image_input)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     53\u001b[0m     text_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_text(text_tokens)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     55\u001b[0m text_probs \u001b[39m=\u001b[39m (\u001b[39m100.0\u001b[39m \u001b[39m*\u001b[39m image_features \u001b[39m@\u001b[39m text_features\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open_clip/model.py:182\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image, normalize)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_image\u001b[39m(\u001b[39mself\u001b[39m, image, normalize: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 182\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisual(image)\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mnormalize(features, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m normalize \u001b[39melse\u001b[39;00m features\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open_clip/modified_resnet.py:174\u001b[0m, in \u001b[0;36mModifiedResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 174\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem(x)\n\u001b[1;32m    175\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[1;32m    176\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/open_clip/modified_resnet.py:167\u001b[0m, in \u001b[0;36mModifiedResNet.stem\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstem\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 167\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    168\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m    169\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from utils.clip_utils import *\n",
    "\n",
    "# len(=nr_of_batches)*batch_size=nr_of_samples\n",
    "# len(val_loader)-1 full batches with a size of 128 images\n",
    "# one last batch with the remaining <128 images\n",
    "#(len(val_loader)-1) * 128 + 16 \n",
    "\n",
    "asses_clip_performance(model, preprocess, train_loader_gray, descriptions, dataset_name=\"Training gray\")\n",
    "asses_clip_performance(model, preprocess, val_loader_gray, descriptions, dataset_name=\"Validation gray\")\n",
    "asses_clip_performance(model, preprocess, test_loader_gray, descriptions, dataset_name=\"Test gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asses_clip_performance(model, preprocess, train_loader_color, descriptions, dataset_name=\"Training color\")\n",
    "asses_clip_performance(model, preprocess, val_loader_color, descriptions, dataset_name=\"Validation color\")\n",
    "asses_clip_performance(model, preprocess, test_loader_color, descriptions, dataset_name=\"Test color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 605M/605M [00:53<00:00, 11.2MiB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CLIP.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model, _, preprocess \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-B-32-quickgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaion400m_e32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mget_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mViT-B-32-quickgelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCLIP.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma diagram\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma dog\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma cat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/PIL/Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2950\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   2952\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 2953\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2954\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2956\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CLIP.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
