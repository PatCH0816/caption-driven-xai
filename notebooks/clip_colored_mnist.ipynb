{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.0a0+d0d6b1f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n"
     ]
    }
   ],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from torchvision import transforms\n",
    "\n",
    "# parameters\n",
    "size_of_batch = 128\n",
    "\n",
    "# dataset preparation\n",
    "train_set = ColoredMNIST(root='./data',\n",
    "                       env='train',\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "val_set = ColoredMNIST(root='./data',\n",
    "                       env='val',\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test_set = ColoredMNIST(root='./data',\n",
    "                       env='test',\n",
    "                       transform= transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_set,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                           batch_size=size_of_batch,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7fc116854d30>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  3306,  1002,   256, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  3306,  1002,   256,   607,  1981,   533,  5286,   269, 49407,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize(\"Hello World! My name is Patrick.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  3306,  1002,   256,   607,  1981,   533,  8625,   269, 49407,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.tokenize(\"Hello World! My name is Michelle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance of the clip model on the colored mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images in skimage to use and their textual descriptions\n",
    "\n",
    "# Results in:\n",
    "# Training accuracy:  33.85\n",
    "# Validation accuracy:  36.16\n",
    "# Test accuracy:  34.16\n",
    "descriptions = {\n",
    "    \"0\": \"a number with the value zero\",\n",
    "    \"1\": \"a number with the value one\",\n",
    "    \"2\": \"a number with the value two\",\n",
    "    \"3\": \"a number with the value three\",\n",
    "    \"4\": \"a number with the value four\",\n",
    "    \"5\": \"a number with the value five\",\n",
    "    \"6\": \"a number with the value six\",\n",
    "    \"7\": \"a number with the value seven\",\n",
    "    \"8\": \"a number with the value eight\",\n",
    "    \"9\": \"a number with the value nine\"\n",
    "}\n",
    "\n",
    "# Results in:\n",
    "# Training accuracy:  33.85\n",
    "# Validation accuracy:  36.16\n",
    "# Test accuracy:  34.16\n",
    "descriptions2 = {\n",
    "    \"0\": 'a photo of the number: \"0\".',\n",
    "    \"1\": 'a photo of the number: \"1\".',\n",
    "    \"2\": 'a photo of the number: \"2\".',\n",
    "    \"3\": 'a photo of the number: \"3\".',\n",
    "    \"4\": 'a photo of the number: \"4\".',\n",
    "    \"5\": 'a photo of the number: \"5\".',\n",
    "    \"6\": 'a photo of the number: \"6\".',\n",
    "    \"7\": 'a photo of the number: \"7\".',\n",
    "    \"8\": 'a photo of the number: \"8\".',\n",
    "    \"9\": 'a photo of the number: \"9\".',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact same text tokens result in the same performance!\n",
    "clip.tokenize(descriptions1) == clip.tokenize(descriptions2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clip_utils import *\n",
    "\n",
    "# len(=nr_of_batches)*batch_size=nr_of_samples\n",
    "# len(val_loader)-1 full batches with a size of 128 images\n",
    "# one last batch with the remaining <128 images\n",
    "#(len(val_loader)-1) * 128 + 16 \n",
    "\n",
    "asses_clip_performance(model, preprocess, train_loader, descriptions, dataset_name=\"Training\")\n",
    "asses_clip_performance(model, preprocess, val_loader, descriptions, dataset_name=\"Validation\")\n",
    "asses_clip_performance(model, preprocess, test_loader, descriptions, dataset_name=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up input images and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist_preprocessing import *\n",
    "from utils.mnist_plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.mnist_general'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist_general\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m original_images, images, texts \u001b[38;5;241m=\u001b[39m show_examples_0_to_9(val_set, preprocess, descriptions)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.mnist_general'"
     ]
    }
   ],
   "source": [
    "from utils.mnist_general import *\n",
    "\n",
    "original_images, images, texts = show_examples_0_to_9(val_set, preprocess, descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input).float()\n",
    "    text_features = model.encode_text(text_tokens).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(descriptions)\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "# plt.colorbar()\n",
    "plt.yticks(range(count), texts, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, count - 0.5])\n",
    "plt.ylim([count + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features\", size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_classes = []\n",
    "number_classes.append(\"zero\")\n",
    "number_classes.append(\"one\")\n",
    "number_classes.append(\"two\")\n",
    "number_classes.append(\"three\")\n",
    "number_classes.append(\"four\")\n",
    "number_classes.append(\"five\")\n",
    "number_classes.append(\"six\")\n",
    "number_classes.append(\"seven\")\n",
    "number_classes.append(\"eight\")\n",
    "number_classes.append(\"nine\")\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in number_classes]\n",
    "\n",
    "text_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_descriptions = []\n",
    "text_descriptions.append('a photo of the number: \"0\".')\n",
    "text_descriptions.append('a photo of the number: \"1\".')\n",
    "text_descriptions.append('a photo of the number: \"2\".')\n",
    "text_descriptions.append('a photo of the number: \"3\".')\n",
    "text_descriptions.append('a photo of the number: \"4\".')\n",
    "text_descriptions.append('a photo of the number: \"5\".')\n",
    "text_descriptions.append('a photo of the number: \"6\".')\n",
    "text_descriptions.append('a photo of the number: \"7\".')\n",
    "text_descriptions.append('a photo of the number: \"8\".')\n",
    "text_descriptions.append('a photo of the number: \"9\".')\n",
    "text_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = clip.tokenize(text_descriptions).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.subplot(5, 4, 2 * i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(5, 4, 2 * i + 2)\n",
    "    y = np.arange(top_probs.shape[-1])\n",
    "    plt.grid()\n",
    "    plt.barh(y, top_probs[i])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.yticks(y, [number_classes[index] for index in top_labels[i].numpy()])\n",
    "    plt.xlabel(\"probability\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
